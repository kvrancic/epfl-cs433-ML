{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from helpers import *\n",
    "\n",
    "height, weight, gender = load_data(sub_sample=False, add_outlier=False)\n",
    "x, mean_x, std_x = standardize(height)\n",
    "y, tx = build_model_data(x, weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10000,), (10000, 2))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape, tx.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NB: throughout this laboratory the data has the following format: \n",
    "  * there are **N = 10000** data entries\n",
    "  * **y** represents the column vector containing weight information -- that which we wish to predict/the output (see also the first page of $\\texttt{exercise02.pdf}$). Its **shape** is **(N,)**.\n",
    "  * **tx** represents the matrix $\\tilde{X}$ formed by laterally concatenating a column vector of 1s to the column vector of height information -- the input data (see also the first page of $\\texttt{exercise02.pdf}$). Its **shape** is **(N,2)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Computing the Cost Function\n",
    "Fill in the `compute_cost` function below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SOLUTION\n",
    "def calculate_mse(e):\n",
    "    \"\"\"Calculate the mse for vector e.\"\"\"\n",
    "    return 1 / 2 * np.mean(e**2)\n",
    "\n",
    "\n",
    "def calculate_mae(e):\n",
    "    \"\"\"Calculate the mae for vector e.\"\"\"\n",
    "    return np.mean(np.abs(e))\n",
    "\n",
    "\n",
    "### TEMPLATE\n",
    "### END SOLUTION\n",
    "\n",
    "\n",
    "def compute_loss(y, tx, w):\n",
    "\n",
    "    \"\"\"Calculate the loss using either MSE or MAE.\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        w: numpy array of shape=(2,). The vector of model parameters.\n",
    "\n",
    "    Returns:\n",
    "        the value of the loss (a scalar), corresponding to the input parameters w.\n",
    "    \"\"\"\n",
    "    ### SOLUTION\n",
    "    e = y - tx.dot(w)\n",
    "    return calculate_mse(e)\n",
    "    ### TEMPLATE\n",
    "    # # ***************************************************\n",
    "    # # INSERT YOUR CODE HERE\n",
    "    # # TODO: compute loss by MSE\n",
    "    # # ***************************************************\n",
    "    # raise NotImplementedError\n",
    "    ### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Grid Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill in the function `grid_search()` below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from costs import *\n",
    "\n",
    "\n",
    "def grid_search(y, tx, grid_w0, grid_w1):\n",
    "    \"\"\"Algorithm for grid search.\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        grid_w0: numpy array of shape=(num_grid_pts_w0, ). A 1D array containing num_grid_pts_w0 values of parameter w0 to be tested in the grid search.\n",
    "        grid_w1: numpy array of shape=(num_grid_pts_w1, ). A 1D array containing num_grid_pts_w1 values of parameter w1 to be tested in the grid search.\n",
    "\n",
    "    Returns:\n",
    "        losses: numpy array of shape=(num_grid_pts_w0, num_grid_pts_w1). A 2D array containing the loss value for each combination of w0 and w1\n",
    "    \"\"\"\n",
    "\n",
    "    losses = np.zeros((len(grid_w0), len(grid_w1)))\n",
    "    ### SOLUTION\n",
    "    # compute loss for each combinationof w0 and w1.\n",
    "    for ind_row, row in enumerate(grid_w0):\n",
    "        for ind_col, col in enumerate(grid_w1):\n",
    "            w = np.array([row, col])\n",
    "            losses[ind_row, ind_col] = compute_loss(y, tx, w)\n",
    "    ### TEMPLATE\n",
    "    # # ***************************************************\n",
    "    # # INSERT YOUR CODE HERE\n",
    "    # # TODO: compute loss for each combination of w0 and w1.\n",
    "    # # ***************************************************\n",
    "    # raise NotImplementedError\n",
    "    ### END SOLUTION\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us play with the grid search demo now!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid Search: loss*=42.42448314678248, w0*=66.66666666666669, w1*=16.666666666666686, execution time=0.027 seconds\n"
     ]
    }
   ],
   "source": [
    "from grid_search import generate_w, get_best_parameters\n",
    "from plots import grid_visualization\n",
    "\n",
    "# Generate the grid of parameters to be swept\n",
    "grid_w0, grid_w1 = generate_w(num_intervals=10)\n",
    "\n",
    "# Start the grid search\n",
    "start_time = datetime.datetime.now()\n",
    "grid_losses = grid_search(y, tx, grid_w0, grid_w1)\n",
    "\n",
    "# Select the best combinaison\n",
    "loss_star, w0_star, w1_star = get_best_parameters(grid_w0, grid_w1, grid_losses)\n",
    "end_time = datetime.datetime.now()\n",
    "execution_time = (end_time - start_time).total_seconds()\n",
    "\n",
    "# Print the results\n",
    "print(\n",
    "    \"Grid Search: loss*={l}, w0*={w0}, w1*={w1}, execution time={t:.3f} seconds\".format(\n",
    "        l=loss_star, w0=w0_star, w1=w1_star, t=execution_time\n",
    "    )\n",
    ")\n",
    "\n",
    "# Plot the results\n",
    "fig = grid_visualization(grid_losses, grid_w0, grid_w1, mean_x, std_x, height, weight)\n",
    "fig.set_size_inches(10.0, 6.0)\n",
    "fig.savefig(\"grid_plot\")  # Optional saving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, please fill in the functions `compute_gradient` below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(y, tx, w):\n",
    "    \"\"\"Computes the gradient at w.\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        w: numpy array of shape=(2, ). The vector of model parameters.\n",
    "\n",
    "    Returns:\n",
    "        An numpy array of shape (2, ) (same shape as w), containing the gradient of the loss at w.\n",
    "    \"\"\"\n",
    "    ### SOLUTION\n",
    "    err = y - tx.dot(w)\n",
    "    grad = -tx.T.dot(err) / len(err)\n",
    "    return grad, err\n",
    "\n",
    "    ### TEMPLATE\n",
    "    # # ***************************************************\n",
    "    # # INSERT YOUR CODE HERE\n",
    "    # # TODO: compute gradient vector\n",
    "    # # ***************************************************\n",
    "    # raise NotImplementedError\n",
    "    ### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please fill in the functions `gradient_descent` below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(y, tx, initial_w, max_iters, gamma):\n",
    "    \"\"\"The Gradient Descent (GD) algorithm.\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        initial_w: numpy array of shape=(2, ). The initial guess (or the initialization) for the model parameters\n",
    "        max_iters: a scalar denoting the total number of iterations of GD\n",
    "        gamma: a scalar denoting the stepsize\n",
    "\n",
    "    Returns:\n",
    "        losses: a list of length max_iters containing the loss value (scalar) for each iteration of GD\n",
    "        ws: a list of length max_iters containing the model parameters as numpy arrays of shape (2, ), for each iteration of GD\n",
    "    \"\"\"\n",
    "    # Define parameters to store w and loss\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        ### SOLUTION\n",
    "        # compute loss, gradient\n",
    "        grad, err = compute_gradient(y, tx, w)\n",
    "        loss = calculate_mse(err)\n",
    "        # update w by gradient descent\n",
    "        w = w - gamma * grad\n",
    "\n",
    "        ### TEMPLATE\n",
    "        # # ***************************************************\n",
    "        # # INSERT YOUR CODE HERE\n",
    "        # # TODO: compute gradient and loss\n",
    "        # # ***************************************************\n",
    "        # raise NotImplementedError\n",
    "        # # ***************************************************\n",
    "        # # INSERT YOUR CODE HERE\n",
    "        # # TODO: update w by gradient\n",
    "        # # ***************************************************\n",
    "        # raise NotImplementedError\n",
    "        ### END SOLUTION\n",
    "\n",
    "        # store w and loss\n",
    "        ws.append(w)\n",
    "        losses.append(loss)\n",
    "        print(\n",
    "            \"GD iter. {bi}/{ti}: loss={l}, w0={w0}, w1={w1}\".format(\n",
    "                bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return losses, ws"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your gradient descent function through gradient descent demo shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GD iter. 0/49: loss=2792.2367127591674, w0=51.305745401473644, w1=9.435798704492269\n",
      "GD iter. 1/49: loss=265.3024621089598, w0=66.69746902191571, w1=12.266538315840005\n",
      "GD iter. 2/49: loss=37.87837955044126, w0=71.31498610804834, w1=13.115760199244333\n",
      "GD iter. 3/49: loss=17.410212120174467, w0=72.70024123388814, w1=13.370526764265632\n",
      "GD iter. 4/49: loss=15.568077051450455, w0=73.11581777164007, w1=13.446956733772023\n",
      "GD iter. 5/49: loss=15.402284895265295, w0=73.24049073296565, w1=13.469885724623941\n",
      "GD iter. 6/49: loss=15.38736360120863, w0=73.27789262136334, w1=13.476764421879516\n",
      "GD iter. 7/49: loss=15.38602068474353, w0=73.28911318788263, w1=13.478828031056189\n",
      "GD iter. 8/49: loss=15.385899822261674, w0=73.29247935783842, w1=13.47944711380919\n",
      "GD iter. 9/49: loss=15.385888944638305, w0=73.29348920882515, w1=13.47963283863509\n",
      "GD iter. 10/49: loss=15.3858879656522, w0=73.29379216412117, w1=13.479688556082861\n",
      "GD iter. 11/49: loss=15.385887877543452, w0=73.29388305070998, w1=13.479705271317192\n",
      "GD iter. 12/49: loss=15.385887869613665, w0=73.29391031668663, w1=13.479710285887492\n",
      "GD iter. 13/49: loss=15.385887868899983, w0=73.29391849647962, w1=13.479711790258582\n",
      "GD iter. 14/49: loss=15.38588786883575, w0=73.29392095041752, w1=13.479712241569908\n",
      "GD iter. 15/49: loss=15.385887868829974, w0=73.29392168659889, w1=13.479712376963306\n",
      "GD iter. 16/49: loss=15.38588786882945, w0=73.2939219074533, w1=13.479712417581325\n",
      "GD iter. 17/49: loss=15.385887868829403, w0=73.29392197370962, w1=13.479712429766732\n",
      "GD iter. 18/49: loss=15.3858878688294, w0=73.29392199358652, w1=13.479712433422353\n",
      "GD iter. 19/49: loss=15.385887868829403, w0=73.2939219995496, w1=13.47971243451904\n",
      "GD iter. 20/49: loss=15.385887868829398, w0=73.29392200133852, w1=13.479712434848047\n",
      "GD iter. 21/49: loss=15.3858878688294, w0=73.29392200187519, w1=13.479712434946748\n",
      "GD iter. 22/49: loss=15.3858878688294, w0=73.29392200203618, w1=13.479712434976358\n",
      "GD iter. 23/49: loss=15.3858878688294, w0=73.29392200208449, w1=13.479712434985242\n",
      "GD iter. 24/49: loss=15.3858878688294, w0=73.29392200209898, w1=13.479712434987906\n",
      "GD iter. 25/49: loss=15.385887868829398, w0=73.29392200210333, w1=13.479712434988706\n",
      "GD iter. 26/49: loss=15.3858878688294, w0=73.29392200210464, w1=13.479712434988945\n",
      "GD iter. 27/49: loss=15.3858878688294, w0=73.29392200210502, w1=13.479712434989018\n",
      "GD iter. 28/49: loss=15.3858878688294, w0=73.29392200210513, w1=13.47971243498904\n",
      "GD iter. 29/49: loss=15.3858878688294, w0=73.29392200210518, w1=13.479712434989047\n",
      "GD iter. 30/49: loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 31/49: loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 32/49: loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 33/49: loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 34/49: loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 35/49: loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 36/49: loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 37/49: loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 38/49: loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 39/49: loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 40/49: loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 41/49: loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 42/49: loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 43/49: loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 44/49: loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 45/49: loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 46/49: loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 47/49: loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 48/49: loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 49/49: loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD: execution time=0.018 seconds\n"
     ]
    }
   ],
   "source": [
    "# from gradient_descent import *\n",
    "from plots import gradient_descent_visualization\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 50\n",
    "gamma = 0.7\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([0, 0])\n",
    "\n",
    "# Start gradient descent.\n",
    "start_time = datetime.datetime.now()\n",
    "gd_losses, gd_ws = gradient_descent(y, tx, w_initial, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"GD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99060eedc3ac43c696ce8169376ebe6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='n_iter', max=51, min=1), Output()), _dom_classes=('widge…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_figure(n_iter)>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Time Visualization\n",
    "from ipywidgets import IntSlider, interact\n",
    "\n",
    "\n",
    "def plot_figure(n_iter):\n",
    "    fig = gradient_descent_visualization(\n",
    "        gd_losses,\n",
    "        gd_ws,\n",
    "        grid_losses,\n",
    "        grid_w0,\n",
    "        grid_w1,\n",
    "        mean_x,\n",
    "        std_x,\n",
    "        height,\n",
    "        weight,\n",
    "        n_iter,\n",
    "    )\n",
    "    fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "\n",
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(gd_ws)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 4. Stochastic gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_stoch_gradient(y, tx, w):\n",
    "    \"\"\"Compute a stochastic gradient at w from just few examples n and their corresponding y_n labels.\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        w: numpy array of shape=(2, ). The vector of model parameters.\n",
    "\n",
    "    Returns:\n",
    "        A numpy array of shape (2, ) (same shape as w), containing the stochastic gradient of the loss at w.\n",
    "    \"\"\"\n",
    "\n",
    "    ### SOLUTION\n",
    "    err = y - tx.dot(w)\n",
    "    grad = -tx.T.dot(err) / len(err)\n",
    "    return grad, err\n",
    "\n",
    "    ### TEMPLATE\n",
    "    # # ***************************************************\n",
    "    # # INSERT YOUR CODE HERE\n",
    "    # # TODO: implement stochastic gradient computation. It's the same as the usual gradient.\n",
    "    # # ***************************************************\n",
    "    # raise NotImplementedError\n",
    "    ### END SOLUTION\n",
    "\n",
    "\n",
    "def stochastic_gradient_descent(y, tx, initial_w, batch_size, max_iters, gamma):\n",
    "    \"\"\"The Stochastic Gradient Descent algorithm (SGD).\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        initial_w: numpy array of shape=(2, ). The initial guess (or the initialization) for the model parameters\n",
    "        batch_size: a scalar denoting the number of data points in a mini-batch used for computing the stochastic gradient\n",
    "        max_iters: a scalar denoting the total number of iterations of SGD\n",
    "        gamma: a scalar denoting the stepsize\n",
    "\n",
    "    Returns:\n",
    "        losses: a list of length max_iters containing the loss value (scalar) for each iteration of SGD\n",
    "        ws: a list of length max_iters containing the model parameters as numpy arrays of shape (2, ), for each iteration of SGD\n",
    "    \"\"\"\n",
    "\n",
    "    # Define parameters to store w and loss\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "\n",
    "    for n_iter in range(max_iters):\n",
    "        ### SOLUTION\n",
    "        for y_batch, tx_batch in batch_iter(\n",
    "            y, tx, batch_size=batch_size, num_batches=1\n",
    "        ):\n",
    "            # compute a stochastic gradient and loss\n",
    "            grad, _ = compute_stoch_gradient(y_batch, tx_batch, w)\n",
    "            # update w through the stochastic gradient update\n",
    "            w = w - gamma * grad\n",
    "            # calculate loss\n",
    "            loss = compute_loss(y, tx, w)\n",
    "            # store w and loss\n",
    "            ws.append(w)\n",
    "            losses.append(loss)\n",
    "\n",
    "        ### TEMPLATE\n",
    "        # # ***************************************************\n",
    "        # # INSERT YOUR CODE HERE\n",
    "        # # TODO: implement stochastic gradient descent.\n",
    "        # # ***************************************************\n",
    "        # raise NotImplementedError\n",
    "        ### END SOLUTION\n",
    "\n",
    "        print(\n",
    "            \"SGD iter. {bi}/{ti}: loss={l}, w0={w0}, w1={w1}\".format(\n",
    "                bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]\n",
    "            )\n",
    "        )\n",
    "    return losses, ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD iter. 0/49: loss=2127.7602707005312, w0=8.612715904649866, w1=7.069537441908745\n",
      "SGD iter. 1/49: loss=1671.1560635302933, w0=15.876288608742268, w1=9.638394063869118\n",
      "SGD iter. 2/49: loss=1314.8277057136315, w0=22.558963958548667, w1=18.46445595980181\n",
      "SGD iter. 3/49: loss=1043.0372198998787, w0=28.056011911123676, w1=10.507481847976702\n",
      "SGD iter. 4/49: loss=835.958407052763, w0=32.819347644527426, w1=15.198394761882463\n",
      "SGD iter. 5/49: loss=679.6596301110673, w0=36.84473239094948, w1=13.543439779940586\n",
      "SGD iter. 6/49: loss=561.9358158090853, w0=40.42174853015157, w1=17.018083120813255\n",
      "SGD iter. 7/49: loss=458.5407699128821, w0=43.74221447735405, w1=17.08614369867228\n",
      "SGD iter. 8/49: loss=349.70419865198454, w0=47.600765956295454, w1=10.564518792710516\n",
      "SGD iter. 9/49: loss=310.55712462516436, w0=49.4250911560246, w1=8.938634638489429\n",
      "SGD iter. 10/49: loss=239.20226714326995, w0=52.25042589792863, w1=11.287902620614837\n",
      "SGD iter. 11/49: loss=193.41687482039038, w0=54.42541110981511, w1=13.276560741290707\n",
      "SGD iter. 12/49: loss=145.89022203066258, w0=57.138463041028835, w1=13.380646393158067\n",
      "SGD iter. 13/49: loss=115.51529011636532, w0=59.14263935085789, w1=13.477758060634848\n",
      "SGD iter. 14/49: loss=99.54485904725047, w0=60.552025863744305, w1=11.037986578946601\n",
      "SGD iter. 15/49: loss=70.2908257946954, w0=62.8570741954325, w1=12.540519317422948\n",
      "SGD iter. 16/49: loss=60.47135461053655, w0=63.83598633039945, w1=12.632135704957832\n",
      "SGD iter. 17/49: loss=55.87670851084619, w0=64.30994693114303, w1=12.960257712838922\n",
      "SGD iter. 18/49: loss=46.57211783558303, w0=65.44959161939799, w1=12.563775329919045\n",
      "SGD iter. 19/49: loss=44.80250665560544, w0=65.70583547691402, w1=12.359810448311523\n",
      "SGD iter. 20/49: loss=30.383569168094702, w0=67.91455949323446, w1=14.508216978822785\n",
      "SGD iter. 21/49: loss=29.084538430848525, w0=68.1711062837725, w1=14.553984393127113\n",
      "SGD iter. 22/49: loss=26.38524293441604, w0=68.82926914292759, w1=14.916926746847416\n",
      "SGD iter. 23/49: loss=24.89184941548685, w0=69.87365712435003, w1=16.184099844238947\n",
      "SGD iter. 24/49: loss=24.92346127348361, w0=69.82318608239473, w1=16.13096477773954\n",
      "SGD iter. 25/49: loss=19.936254063509644, w0=70.72130722489874, w1=15.055271255680763\n",
      "SGD iter. 26/49: loss=20.103195101722743, w0=70.66515290463009, w1=15.068481612039836\n",
      "SGD iter. 27/49: loss=19.329405441891517, w0=70.86651089535582, w1=14.892054626157325\n",
      "SGD iter. 28/49: loss=16.63184010044024, w0=71.8901307132922, w1=12.75771902734987\n",
      "SGD iter. 29/49: loss=16.83147573405327, w0=71.74937347412511, w1=12.768695153087755\n",
      "SGD iter. 30/49: loss=15.718652982044901, w0=72.4888223943458, w1=13.611412275598974\n",
      "SGD iter. 31/49: loss=15.85229744503934, w0=72.32811998421711, w1=13.486466297824263\n",
      "SGD iter. 32/49: loss=15.727979319133999, w0=72.47760399232872, w1=13.346266537623174\n",
      "SGD iter. 33/49: loss=15.434413344174674, w0=73.05124080412301, w1=13.675050057674692\n",
      "SGD iter. 34/49: loss=15.408970878068208, w0=73.10867236025504, w1=13.58856365763726\n",
      "SGD iter. 35/49: loss=15.414549424643896, w0=73.33548222098236, w1=13.243924691804892\n",
      "SGD iter. 36/49: loss=15.50383645540631, w0=73.7711503049355, w1=13.38943331153563\n",
      "SGD iter. 37/49: loss=15.525290633007781, w0=73.81464596472969, w1=13.392236249055951\n",
      "SGD iter. 38/49: loss=15.453914274793084, w0=73.62588365359056, w1=13.640505077452785\n",
      "SGD iter. 39/49: loss=15.77425189928317, w0=74.1703370224631, w1=13.386842779026606\n",
      "SGD iter. 40/49: loss=16.187667934916366, w0=74.46396121439969, w1=12.995389842219309\n",
      "SGD iter. 41/49: loss=15.95715813609656, w0=74.30248574319893, w1=13.125678941255305\n",
      "SGD iter. 42/49: loss=15.43522711427713, w0=73.6075187430002, w1=13.498031141151966\n",
      "SGD iter. 43/49: loss=15.40986179239999, w0=73.46552207162341, w1=13.343693086062052\n",
      "SGD iter. 44/49: loss=15.398488113885435, w0=73.42038707714237, w1=13.38375893110352\n",
      "SGD iter. 45/49: loss=16.043441174824334, w0=72.79500745821029, w1=14.512277633840924\n",
      "SGD iter. 46/49: loss=15.647794083957002, w0=73.4288315978618, w1=14.190776307807791\n",
      "SGD iter. 47/49: loss=15.810893366078968, w0=74.21110173440492, w1=13.385944993103655\n",
      "SGD iter. 48/49: loss=15.620198951614459, w0=73.97020177340212, w1=13.585862500955509\n",
      "SGD iter. 49/49: loss=20.624229489624792, w0=72.71496595448434, w1=10.295141327835994\n",
      "SGD: execution time=0.038 seconds\n"
     ]
    }
   ],
   "source": [
    "# from stochastic_gradient_descent import *\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 50\n",
    "gamma = 0.1\n",
    "batch_size = 1\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([0, 0])\n",
    "\n",
    "# Start SGD.\n",
    "start_time = datetime.datetime.now()\n",
    "sgd_losses, sgd_ws = stochastic_gradient_descent(\n",
    "    y, tx, w_initial, batch_size, max_iters, gamma\n",
    ")\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"SGD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10f2b4ccb38d4367bdd6b4266c44a78f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='n_iter', max=51, min=1), Output()), _dom_classes=('widge…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_figure(n_iter)>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Time Visualization\n",
    "from ipywidgets import IntSlider, interact\n",
    "\n",
    "\n",
    "def plot_figure(n_iter):\n",
    "    fig = gradient_descent_visualization(\n",
    "        sgd_losses,\n",
    "        sgd_ws,\n",
    "        grid_losses,\n",
    "        grid_w0,\n",
    "        grid_w1,\n",
    "        mean_x,\n",
    "        std_x,\n",
    "        height,\n",
    "        weight,\n",
    "        n_iter,\n",
    "    )\n",
    "    fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "\n",
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(sgd_ws)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Effect of Outliers and MAE Cost Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from helpers import *\n",
    "\n",
    "### SOLUTION\n",
    "height, weight, gender = load_data(sub_sample=True, add_outlier=True)\n",
    "### TEMPLATE\n",
    "## ***************************************************\n",
    "## INSERT YOUR CODE HERE\n",
    "## TODO: reload the data by subsampling first, then by subsampling and adding outliers\n",
    "## ***************************************************\n",
    "# raise NotImplementedError\n",
    "### END SOLUTION\n",
    "\n",
    "x, mean_x, std_x = standardize(height)\n",
    "y, tx = build_model_data(x, weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((202,), (202, 2))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape, tx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GD iter. 0/49: loss=2869.8351145358524, w0=51.84746409844842, w1=7.7244264061924195\n",
      "GD iter. 1/49: loss=318.2821247015965, w0=67.40170332798297, w1=10.041754328050114\n",
      "GD iter. 2/49: loss=88.6423556165128, w0=72.06797509684336, w1=10.736952704607411\n",
      "GD iter. 3/49: loss=67.9747763988552, w0=73.46785662750146, w1=10.945512217574597\n",
      "GD iter. 4/49: loss=66.11469426926604, w0=73.88782108669889, w1=11.00808007146475\n",
      "GD iter. 5/49: loss=65.94728687760302, w0=74.01381042445813, w1=11.026850427631798\n",
      "GD iter. 6/49: loss=65.93222021235334, w0=74.0516072257859, w1=11.032481534481914\n",
      "GD iter. 7/49: loss=65.93086421248087, w0=74.06294626618423, w1=11.034170866536945\n",
      "GD iter. 8/49: loss=65.93074217249236, w0=74.06634797830372, w1=11.034677666153454\n",
      "GD iter. 9/49: loss=65.93073118889338, w0=74.06736849193958, w1=11.034829706038408\n",
      "GD iter. 10/49: loss=65.93073020036948, w0=74.06767464603033, w1=11.034875318003895\n",
      "GD iter. 11/49: loss=65.93073011140233, w0=74.06776649225755, w1=11.034889001593541\n",
      "GD iter. 12/49: loss=65.93073010339529, w0=74.06779404612573, w1=11.034893106670431\n",
      "GD iter. 13/49: loss=65.93073010267466, w0=74.06780231228618, w1=11.034894338193501\n",
      "GD iter. 14/49: loss=65.93073010260979, w0=74.06780479213431, w1=11.034894707650421\n",
      "GD iter. 15/49: loss=65.93073010260395, w0=74.06780553608874, w1=11.034894818487496\n",
      "GD iter. 16/49: loss=65.93073010260342, w0=74.06780575927507, w1=11.03489485173862\n",
      "GD iter. 17/49: loss=65.93073010260338, w0=74.06780582623098, w1=11.034894861713957\n",
      "GD iter. 18/49: loss=65.93073010260338, w0=74.06780584631775, w1=11.034894864706557\n",
      "GD iter. 19/49: loss=65.93073010260336, w0=74.06780585234378, w1=11.034894865604338\n",
      "GD iter. 20/49: loss=65.93073010260338, w0=74.06780585415159, w1=11.034894865873675\n",
      "GD iter. 21/49: loss=65.93073010260336, w0=74.06780585469393, w1=11.034894865954474\n",
      "GD iter. 22/49: loss=65.93073010260338, w0=74.06780585485663, w1=11.034894865978712\n",
      "GD iter. 23/49: loss=65.93073010260336, w0=74.06780585490544, w1=11.034894865985985\n",
      "GD iter. 24/49: loss=65.93073010260338, w0=74.0678058549201, w1=11.034894865988166\n",
      "GD iter. 25/49: loss=65.93073010260336, w0=74.06780585492449, w1=11.034894865988822\n",
      "GD iter. 26/49: loss=65.93073010260336, w0=74.06780585492581, w1=11.034894865989015\n",
      "GD iter. 27/49: loss=65.93073010260336, w0=74.06780585492619, w1=11.034894865989076\n",
      "GD iter. 28/49: loss=65.93073010260338, w0=74.06780585492632, w1=11.034894865989099\n",
      "GD iter. 29/49: loss=65.93073010260339, w0=74.06780585492635, w1=11.0348948659891\n",
      "GD iter. 30/49: loss=65.93073010260338, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 31/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 32/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 33/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 34/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 35/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 36/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 37/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 38/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 39/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 40/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 41/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 42/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 43/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 44/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 45/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 46/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 47/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 48/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 49/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD: execution time=0.003 seconds\n"
     ]
    }
   ],
   "source": [
    "from plots import gradient_descent_visualization\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 50\n",
    "gamma = 0.7\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([0, 0])\n",
    "\n",
    "# Start gradient descent.\n",
    "start_time = datetime.datetime.now()\n",
    "\n",
    "### SOLUTION\n",
    "gd_losses, gd_ws = gradient_descent(y, tx, w_initial, max_iters, gamma)\n",
    "### TEMPLATE\n",
    "# # ***************************************************\n",
    "# # INSERT YOUR CODE HERE\n",
    "# # TODO: fit the model to the subsampled data / subsampled data with outliers and visualize the cloud of points\n",
    "# #       and the model fit\n",
    "# # ***************************************************\n",
    "# raise NotImplementedError\n",
    "### END SOLUTION\n",
    "\n",
    "\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"GD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2ef53282a414ed9bc1e6568f0eb74d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='n_iter', max=51, min=1), Output()), _dom_classes=('widge…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_figure(n_iter)>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Time Visualization\n",
    "from ipywidgets import IntSlider, interact\n",
    "\n",
    "\n",
    "def plot_figure(n_iter):\n",
    "    fig = gradient_descent_visualization(\n",
    "        gd_losses,\n",
    "        gd_ws,\n",
    "        grid_losses,\n",
    "        grid_w0,\n",
    "        grid_w1,\n",
    "        mean_x,\n",
    "        std_x,\n",
    "        height,\n",
    "        weight,\n",
    "        n_iter,\n",
    "    )\n",
    "    fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "\n",
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(gd_ws)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 6. Subgradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_subgradient_mae(y, tx, w):\n",
    "    \"\"\"Compute a subgradient of the MAE at w.\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        w: numpy array of shape=(2, ). The vector of model parameters.\n",
    "\n",
    "    Returns:\n",
    "        A numpy array of shape (2, ) (same shape as w), containing the subgradient of the MAE at w.\n",
    "    \"\"\"\n",
    "    ### SOLUTION\n",
    "    err = y - tx.dot(w)\n",
    "    grad = -np.dot(tx.T, np.sign(err)) / len(err)\n",
    "    return grad, err\n",
    "    ### TEMPLATE\n",
    "    # # ***************************************************\n",
    "    # # INSERT YOUR CODE HERE\n",
    "    # # TODO: compute subgradient gradient vector for MAE\n",
    "    # # ***************************************************\n",
    "    # raise NotImplementedError\n",
    "    ### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subgradient_descent(y, tx, initial_w, max_iters, gamma):\n",
    "    \"\"\"The SubGradient Descent (SubGD) algorithm.\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        initial_w: numpy array of shape=(2, ). The initial guess (or the initialization) for the model parameters\n",
    "        max_iters: a scalar denoting the total number of iterations of GD\n",
    "        gamma: a scalar denoting the stepsize\n",
    "\n",
    "    Returns:\n",
    "        losses: a list of length max_iters containing the loss value (scalar) for each iteration of SubGD\n",
    "        ws: a list of length max_iters containing the model parameters as numpy arrays of shape (2, ), for each iteration of SubGD\n",
    "    \"\"\"\n",
    "    # Define parameters to store w and loss\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        ### SOLUTION\n",
    "        # compute loss, gradient\n",
    "        grad, err = compute_subgradient_mae(y, tx, w)\n",
    "        loss = calculate_mae(err)\n",
    "        # gradient w by descent update\n",
    "        w = w - gamma * grad\n",
    "        # store w and loss\n",
    "\n",
    "        ### TEMPLATE\n",
    "        # # ***************************************************\n",
    "        # # INSERT YOUR CODE HERE\n",
    "        # # TODO: compute subgradient and loss\n",
    "        # # ***************************************************\n",
    "        # raise NotImplementedError\n",
    "        # # ***************************************************\n",
    "        # # INSERT YOUR CODE HERE\n",
    "        # # TODO: update w by subgradient\n",
    "        # # ***************************************************\n",
    "        # raise NotImplementedError\n",
    "        ### END SOLUTION\n",
    "\n",
    "        ws.append(w)\n",
    "        losses.append(loss)\n",
    "        print(\n",
    "            \"SubGD iter. {bi}/{ti}: loss={l}, w0={w0}, w1={w1}\".format(\n",
    "                bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return losses, ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SubGD iter. 0/499: loss=74.06780585492638, w0=0.7, w1=6.109524327590712e-16\n",
      "SubGD iter. 1/499: loss=73.36780585492637, w0=1.4, w1=1.2219048655181425e-15\n",
      "SubGD iter. 2/499: loss=72.66780585492637, w0=2.0999999999999996, w1=1.832857298277214e-15\n",
      "SubGD iter. 3/499: loss=71.96780585492638, w0=2.8, w1=2.443809731036285e-15\n",
      "SubGD iter. 4/499: loss=71.26780585492638, w0=3.5, w1=3.054762163795356e-15\n",
      "SubGD iter. 5/499: loss=70.56780585492638, w0=4.2, w1=3.665714596554428e-15\n",
      "SubGD iter. 6/499: loss=69.86780585492637, w0=4.9, w1=4.276667029313499e-15\n",
      "SubGD iter. 7/499: loss=69.16780585492639, w0=5.6000000000000005, w1=4.887619462072571e-15\n",
      "SubGD iter. 8/499: loss=68.46780585492637, w0=6.300000000000001, w1=5.498571894831642e-15\n",
      "SubGD iter. 9/499: loss=67.76780585492638, w0=7.000000000000001, w1=6.109524327590714e-15\n",
      "SubGD iter. 10/499: loss=67.06780585492638, w0=7.700000000000001, w1=6.720476760349785e-15\n",
      "SubGD iter. 11/499: loss=66.36780585492637, w0=8.4, w1=7.331429193108857e-15\n",
      "SubGD iter. 12/499: loss=65.66780585492639, w0=9.1, w1=7.942381625867928e-15\n",
      "SubGD iter. 13/499: loss=64.96780585492638, w0=9.799999999999999, w1=8.553334058627e-15\n",
      "SubGD iter. 14/499: loss=64.26780585492638, w0=10.499999999999998, w1=9.164286491386072e-15\n",
      "SubGD iter. 15/499: loss=63.567805854926384, w0=11.199999999999998, w1=9.775238924145143e-15\n",
      "SubGD iter. 16/499: loss=62.867805854926374, w0=11.899999999999997, w1=1.0386191356904215e-14\n",
      "SubGD iter. 17/499: loss=62.167805854926385, w0=12.599999999999996, w1=1.0997143789663286e-14\n",
      "SubGD iter. 18/499: loss=61.46780585492638, w0=13.299999999999995, w1=1.1608096222422358e-14\n",
      "SubGD iter. 19/499: loss=60.76780585492638, w0=13.999999999999995, w1=1.2219048655181429e-14\n",
      "SubGD iter. 20/499: loss=60.067805854926384, w0=14.699999999999994, w1=1.28300010879405e-14\n",
      "SubGD iter. 21/499: loss=59.36780585492639, w0=15.399999999999993, w1=1.3440953520699572e-14\n",
      "SubGD iter. 22/499: loss=58.667805854926385, w0=16.099999999999994, w1=1.4051905953458644e-14\n",
      "SubGD iter. 23/499: loss=57.96780585492638, w0=16.799999999999994, w1=1.4662858386217714e-14\n",
      "SubGD iter. 24/499: loss=57.26780585492638, w0=17.499999999999993, w1=1.5273810818976784e-14\n",
      "SubGD iter. 25/499: loss=56.567805854926384, w0=18.199999999999992, w1=1.5884763251735854e-14\n",
      "SubGD iter. 26/499: loss=55.867805854926395, w0=18.89999999999999, w1=1.6495715684494924e-14\n",
      "SubGD iter. 27/499: loss=55.167805854926385, w0=19.59999999999999, w1=1.7106668117253994e-14\n",
      "SubGD iter. 28/499: loss=54.46780585492638, w0=20.29999999999999, w1=1.7717620550013064e-14\n",
      "SubGD iter. 29/499: loss=53.767805854926394, w0=20.99999999999999, w1=1.8328572982772134e-14\n",
      "SubGD iter. 30/499: loss=53.06780585492639, w0=21.69999999999999, w1=1.8939525415531204e-14\n",
      "SubGD iter. 31/499: loss=52.367805854926395, w0=22.399999999999988, w1=1.9550477848290273e-14\n",
      "SubGD iter. 32/499: loss=51.667805854926385, w0=23.099999999999987, w1=2.0161430281049343e-14\n",
      "SubGD iter. 33/499: loss=50.96780585492639, w0=23.799999999999986, w1=2.0772382713808413e-14\n",
      "SubGD iter. 34/499: loss=50.267805854926394, w0=24.499999999999986, w1=2.1383335146567483e-14\n",
      "SubGD iter. 35/499: loss=49.56780585492639, w0=25.199999999999985, w1=2.1994287579326553e-14\n",
      "SubGD iter. 36/499: loss=48.867805854926395, w0=25.899999999999984, w1=2.2605240012085623e-14\n",
      "SubGD iter. 37/499: loss=48.1678058549264, w0=26.599999999999984, w1=2.3216192444844693e-14\n",
      "SubGD iter. 38/499: loss=47.4678058549264, w0=27.299999999999983, w1=2.3827144877603763e-14\n",
      "SubGD iter. 39/499: loss=46.7678058549264, w0=27.999999999999982, w1=2.4438097310362833e-14\n",
      "SubGD iter. 40/499: loss=46.067805854926405, w0=28.69999999999998, w1=2.5049049743121903e-14\n",
      "SubGD iter. 41/499: loss=45.367805854926395, w0=29.39999999999998, w1=2.5660002175880973e-14\n",
      "SubGD iter. 42/499: loss=44.6678058549264, w0=30.09999999999998, w1=2.6270954608640043e-14\n",
      "SubGD iter. 43/499: loss=43.9678058549264, w0=30.79999999999998, w1=2.6881907041399113e-14\n",
      "SubGD iter. 44/499: loss=43.2678058549264, w0=31.49999999999998, w1=2.7492859474158183e-14\n",
      "SubGD iter. 45/499: loss=42.567805854926405, w0=32.19999999999998, w1=2.8103811906917253e-14\n",
      "SubGD iter. 46/499: loss=41.867805854926395, w0=32.899999999999984, w1=2.871476433967632e-14\n",
      "SubGD iter. 47/499: loss=41.167805854926385, w0=33.59999999999999, w1=2.9325716772435396e-14\n",
      "SubGD iter. 48/499: loss=40.46780585492639, w0=34.29999999999999, w1=2.993666920519447e-14\n",
      "SubGD iter. 49/499: loss=39.767805854926394, w0=34.99999999999999, w1=3.054762163795354e-14\n",
      "SubGD iter. 50/499: loss=39.067805854926384, w0=35.699999999999996, w1=3.1158574070712615e-14\n",
      "SubGD iter. 51/499: loss=38.36780585492638, w0=36.4, w1=3.176952650347169e-14\n",
      "SubGD iter. 52/499: loss=37.66780585492638, w0=37.1, w1=3.238047893623076e-14\n",
      "SubGD iter. 53/499: loss=36.96780585492638, w0=37.800000000000004, w1=3.2991431368989835e-14\n",
      "SubGD iter. 54/499: loss=36.26780585492637, w0=38.50000000000001, w1=3.360238380174891e-14\n",
      "SubGD iter. 55/499: loss=35.56780585492637, w0=39.20000000000001, w1=3.421333623450798e-14\n",
      "SubGD iter. 56/499: loss=34.86780585492637, w0=39.90000000000001, w1=3.4824288667267054e-14\n",
      "SubGD iter. 57/499: loss=34.16780585492637, w0=40.600000000000016, w1=3.543524110002613e-14\n",
      "SubGD iter. 58/499: loss=33.46780585492636, w0=41.30000000000002, w1=3.60461935327852e-14\n",
      "SubGD iter. 59/499: loss=32.767805854926365, w0=42.00000000000002, w1=3.6657145965544273e-14\n",
      "SubGD iter. 60/499: loss=32.067805854926355, w0=42.700000000000024, w1=3.7268098398303347e-14\n",
      "SubGD iter. 61/499: loss=31.36780585492636, w0=43.40000000000003, w1=3.787905083106242e-14\n",
      "SubGD iter. 62/499: loss=30.66780585492635, w0=44.10000000000003, w1=3.849000326382149e-14\n",
      "SubGD iter. 63/499: loss=29.967805854926347, w0=44.80000000000003, w1=3.9100955696580566e-14\n",
      "SubGD iter. 64/499: loss=29.267805854926348, w0=45.500000000000036, w1=3.971190812933964e-14\n",
      "SubGD iter. 65/499: loss=28.567805854926345, w0=46.20000000000004, w1=4.032286056209871e-14\n",
      "SubGD iter. 66/499: loss=27.867805854926342, w0=46.90000000000004, w1=4.0933812994857785e-14\n",
      "SubGD iter. 67/499: loss=27.17327020966892, w0=47.59306930693074, w1=0.011147845678271063\n",
      "SubGD iter. 68/499: loss=26.490451563751197, w0=48.279207920792125, w1=0.03308574108989941\n",
      "SubGD iter. 69/499: loss=25.81721232277017, w0=48.96534653465351, w1=0.05502363650152776\n",
      "SubGD iter. 70/499: loss=25.15503943465645, w0=49.63069306930698, w1=0.10538326388307814\n",
      "SubGD iter. 71/499: loss=24.524103413894778, w0=50.28910891089114, w1=0.16746568532793435\n",
      "SubGD iter. 72/499: loss=23.899295346035593, w0=50.947524752475296, w1=0.22954810677279056\n",
      "SubGD iter. 73/499: loss=23.284392925657144, w0=51.59207920792084, w1=0.31242512932747524\n",
      "SubGD iter. 74/499: loss=22.686876444181845, w0=52.22277227722777, w1=0.4119501328839991\n",
      "SubGD iter. 75/499: loss=22.10626756964055, w0=52.84653465346539, w1=0.5208167847923756\n",
      "SubGD iter. 76/499: loss=21.537818828008433, w0=53.4564356435644, w1=0.6457900912635992\n",
      "SubGD iter. 77/499: loss=20.986339874628463, w0=54.0594059405941, w1=0.7796904498577214\n",
      "SubGD iter. 78/499: loss=20.445560936620446, w0=54.655445544554496, w1=0.9197570104995693\n",
      "SubGD iter. 79/499: loss=19.91191015895785, w0=55.24455445544559, w1=1.0670920297849913\n",
      "SubGD iter. 80/499: loss=19.389644090563234, w0=55.819801980198065, w1=1.2261255948210765\n",
      "SubGD iter. 81/499: loss=18.887989064395885, w0=56.36732673267331, w1=1.410709342622213\n",
      "SubGD iter. 82/499: loss=18.415960501854236, w0=56.900990099009945, w1=1.605853732220269\n",
      "SubGD iter. 83/499: loss=17.954898543040386, w0=57.42772277227727, w1=1.808762802293962\n",
      "SubGD iter. 84/499: loss=17.505757656579824, w0=57.933663366336674, w1=2.0285064197514697\n",
      "SubGD iter. 85/499: loss=17.07495742693161, w0=58.43267326732677, w1=2.2494370848672776\n",
      "SubGD iter. 86/499: loss=16.652967297509903, w0=58.91089108910895, w1=2.4837982986028337\n",
      "SubGD iter. 87/499: loss=16.24854073149673, w0=59.382178217821824, w1=2.7260245553531504\n",
      "SubGD iter. 88/499: loss=15.849105212654159, w0=59.83960396039608, w1=2.978742333469136\n",
      "SubGD iter. 89/499: loss=15.46691979123133, w0=60.262376237623805, w1=3.251528669355438\n",
      "SubGD iter. 90/499: loss=15.108294621512215, w0=60.67821782178222, w1=3.5270865794242794\n",
      "SubGD iter. 91/499: loss=14.754896345922832, w0=61.087128712871326, w1=3.806459183951815\n",
      "SubGD iter. 92/499: loss=14.40452896162028, w0=61.49603960396043, w1=4.085831788479351\n",
      "SubGD iter. 93/499: loss=14.055787028127279, w0=61.891089108910926, w1=4.373839384328607\n",
      "SubGD iter. 94/499: loss=13.714620911605635, w0=62.27920792079211, w1=4.666037469532047\n",
      "SubGD iter. 95/499: loss=13.381236307284155, w0=62.65346534653469, w1=4.959829093241769\n",
      "SubGD iter. 96/499: loss=13.058821615166238, w0=63.02079207920796, w1=5.25705719205664\n",
      "SubGD iter. 97/499: loss=12.74025172433924, w0=63.38118811881192, w1=5.560434316352406\n",
      "SubGD iter. 98/499: loss=12.42321888875611, w0=63.74158415841588, w1=5.863811440648173\n",
      "SubGD iter. 99/499: loss=12.107561731901173, w0=64.08811881188123, w1=6.172402175278548\n",
      "SubGD iter. 100/499: loss=11.800622097398135, w0=64.42772277227726, w1=6.486369310516498\n",
      "SubGD iter. 101/499: loss=11.495041794646427, w0=64.7673267326733, w1=6.800336445754448\n",
      "SubGD iter. 102/499: loss=11.189461491894715, w0=65.10693069306933, w1=7.114303580992399\n",
      "SubGD iter. 103/499: loss=10.883881189143004, w0=65.44653465346536, w1=7.428270716230349\n",
      "SubGD iter. 104/499: loss=10.584593408313202, w0=65.76534653465349, w1=7.747893210218626\n",
      "SubGD iter. 105/499: loss=10.295816534318941, w0=66.070297029703, w1=8.073669686866905\n",
      "SubGD iter. 106/499: loss=10.01135208122136, w0=66.37524752475251, w1=8.399446163515185\n",
      "SubGD iter. 107/499: loss=9.72808432666813, w0=66.6663366336634, w1=8.73297028041739\n",
      "SubGD iter. 108/499: loss=9.44812546112251, w0=66.9574257425743, w1=9.066494397319596\n",
      "SubGD iter. 109/499: loss=9.17104110409667, w0=67.23465346534658, w1=9.39863031947029\n",
      "SubGD iter. 110/499: loss=8.903656131158964, w0=67.51188118811886, w1=9.730766241620982\n",
      "SubGD iter. 111/499: loss=8.636271158221257, w0=67.78910891089114, w1=10.062902163771675\n",
      "SubGD iter. 112/499: loss=8.376151920302375, w0=68.06633663366343, w1=10.363999289979422\n",
      "SubGD iter. 113/499: loss=8.140540838751496, w0=68.32970297029709, w1=10.660466909273612\n",
      "SubGD iter. 114/499: loss=7.918544501597273, w0=68.59306930693076, w1=10.943174379960814\n",
      "SubGD iter. 115/499: loss=7.705279728377, w0=68.85643564356442, w1=11.225881850648015\n",
      "SubGD iter. 116/499: loss=7.493695831178641, w0=69.11287128712878, w1=11.504395843582206\n",
      "SubGD iter. 117/499: loss=7.289992405743416, w0=69.35544554455453, w1=11.78820189306775\n",
      "SubGD iter. 118/499: loss=7.097234035781543, w0=69.58415841584166, w1=12.060911465190971\n",
      "SubGD iter. 119/499: loss=6.919905294668923, w0=69.80594059405948, w1=12.324245668386048\n",
      "SubGD iter. 120/499: loss=6.750573527315454, w0=70.0277227722773, w1=12.587579871581125\n",
      "SubGD iter. 121/499: loss=6.584744810805664, w0=70.25643564356443, w1=12.824765405096484\n",
      "SubGD iter. 122/499: loss=6.430343276347806, w0=70.47821782178225, w1=13.065616959310148\n",
      "SubGD iter. 123/499: loss=6.278071481890353, w0=70.69306930693077, w1=13.302953389983912\n",
      "SubGD iter. 124/499: loss=6.133663329263324, w0=70.89405940594067, w1=13.525403099312918\n",
      "SubGD iter. 125/499: loss=6.00584079834303, w0=71.08811881188126, w1=13.742945617944212\n",
      "SubGD iter. 126/499: loss=5.885021825223219, w0=71.27524752475254, w1=13.953548196006844\n",
      "SubGD iter. 127/499: loss=5.771635252269658, w0=71.46237623762383, w1=14.164150774069476\n",
      "SubGD iter. 128/499: loss=5.667162061790257, w0=71.62178217821788, w1=14.349779559473173\n",
      "SubGD iter. 129/499: loss=5.586726765993146, w0=71.75346534653471, w1=14.51689010761231\n",
      "SubGD iter. 130/499: loss=5.523847812160388, w0=71.87128712871292, w1=14.670791185324186\n",
      "SubGD iter. 131/499: loss=5.480093708591872, w0=71.95445544554461, w1=14.780276456654521\n",
      "SubGD iter. 132/499: loss=5.4530880035020255, w0=72.0376237623763, w1=14.889761727984856\n",
      "SubGD iter. 133/499: loss=5.427392630862905, w0=72.10693069306937, w1=14.985916181776727\n",
      "SubGD iter. 134/499: loss=5.407322445682752, w0=72.17623762376245, w1=15.082070635568597\n",
      "SubGD iter. 135/499: loss=5.387252260502599, w0=72.24554455445552, w1=15.178225089360467\n",
      "SubGD iter. 136/499: loss=5.3704607803386955, w0=72.30099009900998, w1=15.25972348971591\n",
      "SubGD iter. 137/499: loss=5.357406523334741, w0=72.34950495049513, w1=15.335091856448138\n",
      "SubGD iter. 138/499: loss=5.345929264022584, w0=72.39801980198028, w1=15.410460223180365\n",
      "SubGD iter. 139/499: loss=5.335714659517473, w0=72.43267326732682, w1=15.469961786755725\n",
      "SubGD iter. 140/499: loss=5.330043910465361, w0=72.46039603960405, w1=15.51864528583281\n",
      "SubGD iter. 141/499: loss=5.325676428273225, w0=72.48811881188128, w1=15.561592159086487\n",
      "SubGD iter. 142/499: loss=5.322176726526591, w0=72.5019801980199, w1=15.597828332032526\n",
      "SubGD iter. 143/499: loss=5.320111309643114, w0=72.52277227722782, w1=15.624722856626713\n",
      "SubGD iter. 144/499: loss=5.318478284898438, w0=72.55049504950505, w1=15.642690329098\n",
      "SubGD iter. 145/499: loss=5.3172400485651465, w0=72.56435643564366, w1=15.664356578291091\n",
      "SubGD iter. 146/499: loss=5.316406547951545, w0=72.58514851485158, w1=15.677095775361284\n",
      "SubGD iter. 147/499: loss=5.315557122666144, w0=72.6059405940595, w1=15.689834972431477\n",
      "SubGD iter. 148/499: loss=5.31470769738074, w0=72.62673267326743, w1=15.70257416950167\n",
      "SubGD iter. 149/499: loss=5.313876880922167, w0=72.64059405940604, w1=15.72424041869476\n",
      "SubGD iter. 150/499: loss=5.3130522468713846, w0=72.66138613861396, w1=15.736979615764954\n",
      "SubGD iter. 151/499: loss=5.312377839024387, w0=72.66831683168327, w1=15.74811029423128\n",
      "SubGD iter. 152/499: loss=5.312132229725043, w0=72.67524752475258, w1=15.759240972697606\n",
      "SubGD iter. 153/499: loss=5.311886620425697, w0=72.68217821782189, w1=15.770371651163932\n",
      "SubGD iter. 154/499: loss=5.311683566098433, w0=72.68217821782189, w1=15.774323911906686\n",
      "SubGD iter. 155/499: loss=5.311661251291322, w0=72.68217821782189, w1=15.77827617264944\n",
      "SubGD iter. 156/499: loss=5.311638936484209, w0=72.68217821782189, w1=15.782228433392193\n",
      "SubGD iter. 157/499: loss=5.311616621677096, w0=72.68217821782189, w1=15.786180694134947\n",
      "SubGD iter. 158/499: loss=5.311594306869985, w0=72.68217821782189, w1=15.7901329548777\n",
      "SubGD iter. 159/499: loss=5.311571992062872, w0=72.68217821782189, w1=15.794085215620454\n",
      "SubGD iter. 160/499: loss=5.311549677255758, w0=72.68217821782189, w1=15.798037476363207\n",
      "SubGD iter. 161/499: loss=5.311527362448647, w0=72.68217821782189, w1=15.801989737105961\n",
      "SubGD iter. 162/499: loss=5.311505047641534, w0=72.68217821782189, w1=15.805941997848715\n",
      "SubGD iter. 163/499: loss=5.311482732834422, w0=72.68217821782189, w1=15.809894258591468\n",
      "SubGD iter. 164/499: loss=5.311460418027309, w0=72.68217821782189, w1=15.813846519334222\n",
      "SubGD iter. 165/499: loss=5.311438103220198, w0=72.68217821782189, w1=15.817798780076975\n",
      "SubGD iter. 166/499: loss=5.311415788413085, w0=72.68217821782189, w1=15.821751040819729\n",
      "SubGD iter. 167/499: loss=5.311393473605972, w0=72.68217821782189, w1=15.825703301562482\n",
      "SubGD iter. 168/499: loss=5.31137115879886, w0=72.68217821782189, w1=15.829655562305236\n",
      "SubGD iter. 169/499: loss=5.311348843991747, w0=72.68217821782189, w1=15.83360782304799\n",
      "SubGD iter. 170/499: loss=5.311326529184636, w0=72.68217821782189, w1=15.837560083790743\n",
      "SubGD iter. 171/499: loss=5.311304214377523, w0=72.68217821782189, w1=15.841512344533497\n",
      "SubGD iter. 172/499: loss=5.311281899570409, w0=72.68217821782189, w1=15.84546460527625\n",
      "SubGD iter. 173/499: loss=5.311259584763298, w0=72.68217821782189, w1=15.849416866019004\n",
      "SubGD iter. 174/499: loss=5.311237269956185, w0=72.68217821782189, w1=15.853369126761757\n",
      "SubGD iter. 175/499: loss=5.311214955149072, w0=72.68217821782189, w1=15.857321387504511\n",
      "SubGD iter. 176/499: loss=5.31119264034196, w0=72.68217821782189, w1=15.861273648247264\n",
      "SubGD iter. 177/499: loss=5.311170325534848, w0=72.68217821782189, w1=15.865225908990018\n",
      "SubGD iter. 178/499: loss=5.311148010727736, w0=72.68217821782189, w1=15.869178169732772\n",
      "SubGD iter. 179/499: loss=5.311125695920624, w0=72.68217821782189, w1=15.873130430475525\n",
      "SubGD iter. 180/499: loss=5.31110338111351, w0=72.68217821782189, w1=15.877082691218279\n",
      "SubGD iter. 181/499: loss=5.311081066306399, w0=72.68217821782189, w1=15.881034951961032\n",
      "SubGD iter. 182/499: loss=5.311058751499285, w0=72.68217821782189, w1=15.884987212703786\n",
      "SubGD iter. 183/499: loss=5.3110364366921745, w0=72.68217821782189, w1=15.88893947344654\n",
      "SubGD iter. 184/499: loss=5.311014121885061, w0=72.68217821782189, w1=15.892891734189293\n",
      "SubGD iter. 185/499: loss=5.310991807077948, w0=72.68217821782189, w1=15.896843994932047\n",
      "SubGD iter. 186/499: loss=5.310969492270837, w0=72.68217821782189, w1=15.9007962556748\n",
      "SubGD iter. 187/499: loss=5.310947177463723, w0=72.68217821782189, w1=15.904748516417554\n",
      "SubGD iter. 188/499: loss=5.310924862656612, w0=72.68217821782189, w1=15.908700777160307\n",
      "SubGD iter. 189/499: loss=5.310902547849499, w0=72.68217821782189, w1=15.91265303790306\n",
      "SubGD iter. 190/499: loss=5.310913706061381, w0=72.67524752475258, w1=15.910526938117323\n",
      "SubGD iter. 191/499: loss=5.310892237186269, w0=72.67524752475258, w1=15.914479198860077\n",
      "SubGD iter. 192/499: loss=5.3108699223791564, w0=72.67524752475258, w1=15.91843145960283\n",
      "SubGD iter. 193/499: loss=5.310862636053835, w0=72.66831683168327, w1=15.916305359817093\n",
      "SubGD iter. 194/499: loss=5.310859611715927, w0=72.66831683168327, w1=15.920257620559847\n",
      "SubGD iter. 195/499: loss=5.310837296908816, w0=72.66831683168327, w1=15.9242098813026\n",
      "SubGD iter. 196/499: loss=5.310814982101703, w0=72.66831683168327, w1=15.928162142045354\n",
      "SubGD iter. 197/499: loss=5.310823570190169, w0=72.66138613861396, w1=15.926036042259616\n",
      "SubGD iter. 198/499: loss=5.310804671438473, w0=72.66138613861396, w1=15.92998830300237\n",
      "SubGD iter. 199/499: loss=5.3107823566313614, w0=72.66138613861396, w1=15.933940563745123\n",
      "SubGD iter. 200/499: loss=5.310772500182623, w0=72.65445544554466, w1=15.931814463959386\n",
      "SubGD iter. 201/499: loss=5.3107720459681325, w0=72.65445544554466, w1=15.93576672470214\n",
      "SubGD iter. 202/499: loss=5.310749731161021, w0=72.65445544554466, w1=15.939718985444893\n",
      "SubGD iter. 203/499: loss=5.310727416353908, w0=72.65445544554466, w1=15.943671246187646\n",
      "SubGD iter. 204/499: loss=5.310733434318958, w0=72.64752475247535, w1=15.941545146401909\n",
      "SubGD iter. 205/499: loss=5.310717105690679, w0=72.64752475247535, w1=15.945497407144662\n",
      "SubGD iter. 206/499: loss=5.3106947908835656, w0=72.64752475247535, w1=15.949449667887416\n",
      "SubGD iter. 207/499: loss=5.310682364311412, w0=72.64059405940604, w1=15.947323568101679\n",
      "SubGD iter. 208/499: loss=5.310684480220337, w0=72.64059405940604, w1=15.951275828844432\n",
      "SubGD iter. 209/499: loss=5.310662165413225, w0=72.64059405940604, w1=15.955228089587186\n",
      "SubGD iter. 210/499: loss=5.310639850606112, w0=72.64059405940604, w1=15.95918035032994\n",
      "SubGD iter. 211/499: loss=5.310643298447746, w0=72.63366336633673, w1=15.957054250544202\n",
      "SubGD iter. 212/499: loss=5.310629539942882, w0=72.63366336633673, w1=15.961006511286955\n",
      "SubGD iter. 213/499: loss=5.3106072251357705, w0=72.63366336633673, w1=15.964958772029709\n",
      "SubGD iter. 214/499: loss=5.3105922284402, w0=72.62673267326743, w1=15.962832672243971\n",
      "SubGD iter. 215/499: loss=5.310633183099026, w0=72.63366336633673, w1=15.967301372051375\n",
      "SubGD iter. 216/499: loss=5.3105993435850625, w0=72.62673267326743, w1=15.965175272265638\n",
      "SubGD iter. 217/499: loss=5.31061822827579, w0=72.63366336633673, w1=15.969643972073042\n",
      "SubGD iter. 218/499: loss=5.310606458729927, w0=72.62673267326743, w1=15.967517872287305\n",
      "SubGD iter. 219/499: loss=5.3106032734525535, w0=72.63366336633673, w1=15.971986572094709\n",
      "SubGD iter. 220/499: loss=5.310613573874789, w0=72.62673267326743, w1=15.969860472308971\n",
      "SubGD iter. 221/499: loss=5.310588318629318, w0=72.63366336633673, w1=15.974329172116375\n",
      "SubGD iter. 222/499: loss=5.310620689019651, w0=72.62673267326743, w1=15.972203072330638\n",
      "SubGD iter. 223/499: loss=5.3105749661498844, w0=72.62673267326743, w1=15.970593411609551\n",
      "SubGD iter. 224/499: loss=5.31058363964973, w0=72.63366336633673, w1=15.975062111416955\n",
      "SubGD iter. 225/499: loss=5.310622915165495, w0=72.62673267326743, w1=15.972936011631218\n",
      "SubGD iter. 226/499: loss=5.310576651555033, w0=72.62673267326743, w1=15.97132635091013\n",
      "SubGD iter. 227/499: loss=5.310578960670142, w0=72.63366336633673, w1=15.975795050717535\n",
      "SubGD iter. 228/499: loss=5.310625141311338, w0=72.62673267326743, w1=15.973668950931797\n",
      "SubGD iter. 229/499: loss=5.31057833696018, w0=72.62673267326743, w1=15.97205929021071\n",
      "SubGD iter. 230/499: loss=5.310574635520699, w0=72.62673267326743, w1=15.970449629489623\n",
      "SubGD iter. 231/499: loss=5.310584557534202, w0=72.63366336633673, w1=15.974918329297028\n",
      "SubGD iter. 232/499: loss=5.31062247845816, w0=72.62673267326743, w1=15.97279222951129\n",
      "SubGD iter. 233/499: loss=5.310576320925847, w0=72.62673267326743, w1=15.971182568790203\n",
      "SubGD iter. 234/499: loss=5.3105798785546146, w0=72.63366336633673, w1=15.975651268597607\n",
      "SubGD iter. 235/499: loss=5.310624704604003, w0=72.62673267326743, w1=15.97352516881187\n",
      "SubGD iter. 236/499: loss=5.310578006330994, w0=72.62673267326743, w1=15.971915508090783\n",
      "SubGD iter. 237/499: loss=5.310575199575027, w0=72.63366336633673, w1=15.976384207898187\n",
      "SubGD iter. 238/499: loss=5.310626930749847, w0=72.62673267326743, w1=15.97425810811245\n",
      "SubGD iter. 239/499: loss=5.31057969173614, w0=72.62673267326743, w1=15.972648447391363\n",
      "SubGD iter. 240/499: loss=5.310575990296659, w0=72.62673267326743, w1=15.971038786670276\n",
      "SubGD iter. 241/499: loss=5.310580796439088, w0=72.63366336633673, w1=15.97550748647768\n",
      "SubGD iter. 242/499: loss=5.310624267896667, w0=72.62673267326743, w1=15.973381386691942\n",
      "SubGD iter. 243/499: loss=5.310577675701808, w0=72.62673267326743, w1=15.971771725970855\n",
      "SubGD iter. 244/499: loss=5.310576117459501, w0=72.63366336633673, w1=15.97624042577826\n",
      "SubGD iter. 245/499: loss=5.31062649404251, w0=72.62673267326743, w1=15.974114325992522\n",
      "SubGD iter. 246/499: loss=5.310579361106954, w0=72.62673267326743, w1=15.972504665271435\n",
      "SubGD iter. 247/499: loss=5.310575659667472, w0=72.62673267326743, w1=15.970895004550348\n",
      "SubGD iter. 248/499: loss=5.310581714323563, w0=72.63366336633673, w1=15.975363704357752\n",
      "SubGD iter. 249/499: loss=5.310623831189332, w0=72.62673267326743, w1=15.973237604572015\n",
      "SubGD iter. 250/499: loss=5.310577345072619, w0=72.62673267326743, w1=15.971627943850928\n",
      "SubGD iter. 251/499: loss=5.310577035343975, w0=72.63366336633673, w1=15.976096643658332\n",
      "SubGD iter. 252/499: loss=5.310626057335176, w0=72.62673267326743, w1=15.973970543872595\n",
      "SubGD iter. 253/499: loss=5.310579030477766, w0=72.62673267326743, w1=15.972360883151508\n",
      "SubGD iter. 254/499: loss=5.3105753290382856, w0=72.62673267326743, w1=15.97075122243042\n",
      "SubGD iter. 255/499: loss=5.310582632208036, w0=72.63366336633673, w1=15.975219922237825\n",
      "SubGD iter. 256/499: loss=5.310623394481999, w0=72.62673267326743, w1=15.973093822452087\n",
      "SubGD iter. 257/499: loss=5.3105770144434326, w0=72.62673267326743, w1=15.971484161731\n",
      "SubGD iter. 258/499: loss=5.3105779532284485, w0=72.63366336633673, w1=15.975952861538405\n",
      "SubGD iter. 259/499: loss=5.3106256206278415, w0=72.62673267326743, w1=15.973826761752667\n",
      "SubGD iter. 260/499: loss=5.31057869984858, w0=72.62673267326743, w1=15.97221710103158\n",
      "SubGD iter. 261/499: loss=5.310574998409098, w0=72.62673267326743, w1=15.970607440310493\n",
      "SubGD iter. 262/499: loss=5.3105835500925105, w0=72.63366336633673, w1=15.975076140117897\n",
      "SubGD iter. 263/499: loss=5.3106229577746635, w0=72.62673267326743, w1=15.97295004033216\n",
      "SubGD iter. 264/499: loss=5.310576683814244, w0=72.62673267326743, w1=15.971340379611073\n",
      "SubGD iter. 265/499: loss=5.310578871112923, w0=72.63366336633673, w1=15.975809079418477\n",
      "SubGD iter. 266/499: loss=5.310625183920505, w0=72.62673267326743, w1=15.97368297963274\n",
      "SubGD iter. 267/499: loss=5.310578369219393, w0=72.62673267326743, w1=15.972073318911653\n",
      "SubGD iter. 268/499: loss=5.310574667779911, w0=72.62673267326743, w1=15.970463658190566\n",
      "SubGD iter. 269/499: loss=5.310584467976984, w0=72.63366336633673, w1=15.97493235799797\n",
      "SubGD iter. 270/499: loss=5.3106225210673275, w0=72.62673267326743, w1=15.972806258212232\n",
      "SubGD iter. 271/499: loss=5.310576353185058, w0=72.62673267326743, w1=15.971196597491145\n",
      "SubGD iter. 272/499: loss=5.310579788997396, w0=72.63366336633673, w1=15.97566529729855\n",
      "SubGD iter. 273/499: loss=5.310624747213171, w0=72.62673267326743, w1=15.973539197512812\n",
      "SubGD iter. 274/499: loss=5.310578038590206, w0=72.62673267326743, w1=15.971929536791725\n",
      "SubGD iter. 275/499: loss=5.310575110017808, w0=72.63366336633673, w1=15.97639823659913\n",
      "SubGD iter. 276/499: loss=5.310626973359014, w0=72.62673267326743, w1=15.974272136813392\n",
      "SubGD iter. 277/499: loss=5.310579723995353, w0=72.62673267326743, w1=15.972662476092305\n",
      "SubGD iter. 278/499: loss=5.310576022555871, w0=72.62673267326743, w1=15.971052815371218\n",
      "SubGD iter. 279/499: loss=5.31058070688187, w0=72.63366336633673, w1=15.975521515178622\n",
      "SubGD iter. 280/499: loss=5.310624310505837, w0=72.62673267326743, w1=15.973395415392885\n",
      "SubGD iter. 281/499: loss=5.310577707961019, w0=72.62673267326743, w1=15.971785754671798\n",
      "SubGD iter. 282/499: loss=5.310576027902282, w0=72.63366336633673, w1=15.976254454479202\n",
      "SubGD iter. 283/499: loss=5.31062653665168, w0=72.62673267326743, w1=15.974128354693464\n",
      "SubGD iter. 284/499: loss=5.310579393366166, w0=72.62673267326743, w1=15.972518693972377\n",
      "SubGD iter. 285/499: loss=5.310575691926685, w0=72.62673267326743, w1=15.97090903325129\n",
      "SubGD iter. 286/499: loss=5.3105816247663435, w0=72.63366336633673, w1=15.975377733058695\n",
      "SubGD iter. 287/499: loss=5.310623873798502, w0=72.62673267326743, w1=15.973251633272957\n",
      "SubGD iter. 288/499: loss=5.310577377331832, w0=72.62673267326743, w1=15.97164197255187\n",
      "SubGD iter. 289/499: loss=5.310576945786757, w0=72.63366336633673, w1=15.976110672359274\n",
      "SubGD iter. 290/499: loss=5.3106260999443435, w0=72.62673267326743, w1=15.973984572573537\n",
      "SubGD iter. 291/499: loss=5.310579062736979, w0=72.62673267326743, w1=15.97237491185245\n",
      "SubGD iter. 292/499: loss=5.310575361297499, w0=72.62673267326743, w1=15.970765251131363\n",
      "SubGD iter. 293/499: loss=5.310582542650818, w0=72.63366336633673, w1=15.975233950938767\n",
      "SubGD iter. 294/499: loss=5.310623437091167, w0=72.62673267326743, w1=15.97310785115303\n",
      "SubGD iter. 295/499: loss=5.310577046702645, w0=72.62673267326743, w1=15.971498190431943\n",
      "SubGD iter. 296/499: loss=5.31057786367123, w0=72.63366336633673, w1=15.975966890239347\n",
      "SubGD iter. 297/499: loss=5.310625663237008, w0=72.62673267326743, w1=15.97384079045361\n",
      "SubGD iter. 298/499: loss=5.310578732107793, w0=72.62673267326743, w1=15.972231129732522\n",
      "SubGD iter. 299/499: loss=5.310575030668312, w0=72.62673267326743, w1=15.970621469011435\n",
      "SubGD iter. 300/499: loss=5.31058346053529, w0=72.63366336633673, w1=15.97509016881884\n",
      "SubGD iter. 301/499: loss=5.310623000383831, w0=72.62673267326743, w1=15.972964069033102\n",
      "SubGD iter. 302/499: loss=5.3105767160734585, w0=72.62673267326743, w1=15.971354408312015\n",
      "SubGD iter. 303/499: loss=5.310578781555702, w0=72.63366336633673, w1=15.97582310811942\n",
      "SubGD iter. 304/499: loss=5.310625226529674, w0=72.62673267326743, w1=15.973697008333682\n",
      "SubGD iter. 305/499: loss=5.3105784014786055, w0=72.62673267326743, w1=15.972087347612595\n",
      "SubGD iter. 306/499: loss=5.310574700039124, w0=72.62673267326743, w1=15.970477686891508\n",
      "SubGD iter. 307/499: loss=5.310584378419764, w0=72.63366336633673, w1=15.974946386698912\n",
      "SubGD iter. 308/499: loss=5.310622563676499, w0=72.62673267326743, w1=15.972820286913175\n",
      "SubGD iter. 309/499: loss=5.3105763854442705, w0=72.62673267326743, w1=15.971210626192088\n",
      "SubGD iter. 310/499: loss=5.310579699440178, w0=72.63366336633673, w1=15.975679325999492\n",
      "SubGD iter. 311/499: loss=5.31062478982234, w0=72.62673267326743, w1=15.973553226213754\n",
      "SubGD iter. 312/499: loss=5.3105780708494175, w0=72.62673267326743, w1=15.971943565492667\n",
      "SubGD iter. 313/499: loss=5.310575020460591, w0=72.63366336633673, w1=15.976412265300072\n",
      "SubGD iter. 314/499: loss=5.3106270159681825, w0=72.62673267326743, w1=15.974286165514334\n",
      "SubGD iter. 315/499: loss=5.310579756254566, w0=72.62673267326743, w1=15.972676504793247\n",
      "SubGD iter. 316/499: loss=5.310576054815085, w0=72.62673267326743, w1=15.97106684407216\n",
      "SubGD iter. 317/499: loss=5.310580617324651, w0=72.63366336633673, w1=15.975535543879564\n",
      "SubGD iter. 318/499: loss=5.3106243531150055, w0=72.62673267326743, w1=15.973409444093827\n",
      "SubGD iter. 319/499: loss=5.310577740220231, w0=72.62673267326743, w1=15.97179978337274\n",
      "SubGD iter. 320/499: loss=5.310575938345063, w0=72.63366336633673, w1=15.976268483180144\n",
      "SubGD iter. 321/499: loss=5.310626579260847, w0=72.62673267326743, w1=15.974142383394407\n",
      "SubGD iter. 322/499: loss=5.31057942562538, w0=72.62673267326743, w1=15.97253272267332\n",
      "SubGD iter. 323/499: loss=5.310575724185897, w0=72.62673267326743, w1=15.970923061952233\n",
      "SubGD iter. 324/499: loss=5.310581535209124, w0=72.63366336633673, w1=15.975391761759637\n",
      "SubGD iter. 325/499: loss=5.310623916407671, w0=72.62673267326743, w1=15.9732656619739\n",
      "SubGD iter. 326/499: loss=5.310577409591045, w0=72.62673267326743, w1=15.971656001252812\n",
      "SubGD iter. 327/499: loss=5.310576856229536, w0=72.63366336633673, w1=15.976124701060217\n",
      "SubGD iter. 328/499: loss=5.310626142553512, w0=72.62673267326743, w1=15.973998601274479\n",
      "SubGD iter. 329/499: loss=5.310579094996192, w0=72.62673267326743, w1=15.972388940553392\n",
      "SubGD iter. 330/499: loss=5.31057539355671, w0=72.62673267326743, w1=15.970779279832305\n",
      "SubGD iter. 331/499: loss=5.310582453093599, w0=72.63366336633673, w1=15.97524797963971\n",
      "SubGD iter. 332/499: loss=5.310623479700335, w0=72.62673267326743, w1=15.973121879853972\n",
      "SubGD iter. 333/499: loss=5.310577078961858, w0=72.62673267326743, w1=15.971512219132885\n",
      "SubGD iter. 334/499: loss=5.3105777741140106, w0=72.63366336633673, w1=15.975980918940289\n",
      "SubGD iter. 335/499: loss=5.310625705846178, w0=72.62673267326743, w1=15.973854819154552\n",
      "SubGD iter. 336/499: loss=5.310578764367005, w0=72.62673267326743, w1=15.972245158433465\n",
      "SubGD iter. 337/499: loss=5.310575062927524, w0=72.62673267326743, w1=15.970635497712378\n",
      "SubGD iter. 338/499: loss=5.3105833709780725, w0=72.63366336633673, w1=15.975104197519782\n",
      "SubGD iter. 339/499: loss=5.310623042993, w0=72.62673267326743, w1=15.972978097734044\n",
      "SubGD iter. 340/499: loss=5.31057674833267, w0=72.62673267326743, w1=15.971368437012957\n",
      "SubGD iter. 341/499: loss=5.310578691998486, w0=72.63366336633673, w1=15.975837136820362\n",
      "SubGD iter. 342/499: loss=5.310625269138844, w0=72.62673267326743, w1=15.973711037034624\n",
      "SubGD iter. 343/499: loss=5.310578433737819, w0=72.62673267326743, w1=15.972101376313537\n",
      "SubGD iter. 344/499: loss=5.3105747322983365, w0=72.62673267326743, w1=15.97049171559245\n",
      "SubGD iter. 345/499: loss=5.310584288862546, w0=72.63366336633673, w1=15.974960415399854\n",
      "SubGD iter. 346/499: loss=5.3106226062856665, w0=72.62673267326743, w1=15.972834315614117\n",
      "SubGD iter. 347/499: loss=5.310576417703483, w0=72.62673267326743, w1=15.97122465489303\n",
      "SubGD iter. 348/499: loss=5.310579609882959, w0=72.63366336633673, w1=15.975693354700434\n",
      "SubGD iter. 349/499: loss=5.310624832431509, w0=72.62673267326743, w1=15.973567254914697\n",
      "SubGD iter. 350/499: loss=5.3105781031086305, w0=72.62673267326743, w1=15.97195759419361\n",
      "SubGD iter. 351/499: loss=5.310574930903372, w0=72.63366336633673, w1=15.976426294001014\n",
      "SubGD iter. 352/499: loss=5.310627058577351, w0=72.62673267326743, w1=15.974300194215276\n",
      "SubGD iter. 353/499: loss=5.3105797885137775, w0=72.62673267326743, w1=15.97269053349419\n",
      "SubGD iter. 354/499: loss=5.310576087074297, w0=72.62673267326743, w1=15.971080872773102\n",
      "SubGD iter. 355/499: loss=5.310580527767432, w0=72.63366336633673, w1=15.975549572580507\n",
      "SubGD iter. 356/499: loss=5.310624395724173, w0=72.62673267326743, w1=15.973423472794769\n",
      "SubGD iter. 357/499: loss=5.310577772479444, w0=72.62673267326743, w1=15.971813812073682\n",
      "SubGD iter. 358/499: loss=5.3105758487878445, w0=72.63366336633673, w1=15.976282511881086\n",
      "SubGD iter. 359/499: loss=5.310626621870017, w0=72.62673267326743, w1=15.974156412095349\n",
      "SubGD iter. 360/499: loss=5.31057945788459, w0=72.62673267326743, w1=15.972546751374262\n",
      "SubGD iter. 361/499: loss=5.31057575644511, w0=72.62673267326743, w1=15.970937090653175\n",
      "SubGD iter. 362/499: loss=5.310581445651906, w0=72.63366336633673, w1=15.975405790460579\n",
      "SubGD iter. 363/499: loss=5.310623959016839, w0=72.62673267326743, w1=15.973279690674842\n",
      "SubGD iter. 364/499: loss=5.310577441850257, w0=72.62673267326743, w1=15.971670029953755\n",
      "SubGD iter. 365/499: loss=5.310576766672319, w0=72.63366336633673, w1=15.976138729761159\n",
      "SubGD iter. 366/499: loss=5.310626185162682, w0=72.62673267326743, w1=15.974012629975421\n",
      "SubGD iter. 367/499: loss=5.310579127255404, w0=72.62673267326743, w1=15.972402969254334\n",
      "SubGD iter. 368/499: loss=5.310575425815924, w0=72.62673267326743, w1=15.970793308533247\n",
      "SubGD iter. 369/499: loss=5.310582363536381, w0=72.63366336633673, w1=15.975262008340652\n",
      "SubGD iter. 370/499: loss=5.310623522309504, w0=72.62673267326743, w1=15.973135908554914\n",
      "SubGD iter. 371/499: loss=5.31057711122107, w0=72.62673267326743, w1=15.971526247833827\n",
      "SubGD iter. 372/499: loss=5.310577684556793, w0=72.63366336633673, w1=15.975994947641231\n",
      "SubGD iter. 373/499: loss=5.310625748455347, w0=72.62673267326743, w1=15.973868847855494\n",
      "SubGD iter. 374/499: loss=5.310578796626218, w0=72.62673267326743, w1=15.972259187134407\n",
      "SubGD iter. 375/499: loss=5.310575095186736, w0=72.62673267326743, w1=15.97064952641332\n",
      "SubGD iter. 376/499: loss=5.310583281420854, w0=72.63366336633673, w1=15.975118226220724\n",
      "SubGD iter. 377/499: loss=5.3106230856021694, w0=72.62673267326743, w1=15.972992126434987\n",
      "SubGD iter. 378/499: loss=5.310576780591885, w0=72.62673267326743, w1=15.9713824657139\n",
      "SubGD iter. 379/499: loss=5.310578602441266, w0=72.63366336633673, w1=15.975851165521304\n",
      "SubGD iter. 380/499: loss=5.310625311748012, w0=72.62673267326743, w1=15.973725065735566\n",
      "SubGD iter. 381/499: loss=5.310578465997031, w0=72.62673267326743, w1=15.97211540501448\n",
      "SubGD iter. 382/499: loss=5.31057476455755, w0=72.62673267326743, w1=15.970505744293392\n",
      "SubGD iter. 383/499: loss=5.310584199305326, w0=72.63366336633673, w1=15.974974444100797\n",
      "SubGD iter. 384/499: loss=5.310622648894835, w0=72.62673267326743, w1=15.972848344315059\n",
      "SubGD iter. 385/499: loss=5.310576449962697, w0=72.62673267326743, w1=15.971238683593972\n",
      "SubGD iter. 386/499: loss=5.31057952032574, w0=72.63366336633673, w1=15.975707383401376\n",
      "SubGD iter. 387/499: loss=5.310624875040677, w0=72.62673267326743, w1=15.973581283615639\n",
      "SubGD iter. 388/499: loss=5.3105781353678445, w0=72.62673267326743, w1=15.971971622894552\n",
      "SubGD iter. 389/499: loss=5.310574841346153, w0=72.63366336633673, w1=15.976440322701956\n",
      "SubGD iter. 390/499: loss=5.31062710118652, w0=72.62673267326743, w1=15.974314222916218\n",
      "SubGD iter. 391/499: loss=5.3105798207729915, w0=72.62673267326743, w1=15.972704562195132\n",
      "SubGD iter. 392/499: loss=5.3105761193335095, w0=72.62673267326743, w1=15.971094901474045\n",
      "SubGD iter. 393/499: loss=5.310580438210215, w0=72.63366336633673, w1=15.975563601281449\n",
      "SubGD iter. 394/499: loss=5.310624438333343, w0=72.62673267326743, w1=15.973437501495711\n",
      "SubGD iter. 395/499: loss=5.3105778047386565, w0=72.62673267326743, w1=15.971827840774624\n",
      "SubGD iter. 396/499: loss=5.310575759230627, w0=72.63366336633673, w1=15.976296540582029\n",
      "SubGD iter. 397/499: loss=5.310626664479185, w0=72.62673267326743, w1=15.974170440796291\n",
      "SubGD iter. 398/499: loss=5.3105794901438035, w0=72.62673267326743, w1=15.972560780075204\n",
      "SubGD iter. 399/499: loss=5.310575788704323, w0=72.62673267326743, w1=15.970951119354117\n",
      "SubGD iter. 400/499: loss=5.310581356094687, w0=72.63366336633673, w1=15.975419819161521\n",
      "SubGD iter. 401/499: loss=5.310624001626008, w0=72.62673267326743, w1=15.973293719375784\n",
      "SubGD iter. 402/499: loss=5.31057747410947, w0=72.62673267326743, w1=15.971684058654697\n",
      "SubGD iter. 403/499: loss=5.310576677115099, w0=72.63366336633673, w1=15.976152758462101\n",
      "SubGD iter. 404/499: loss=5.31062622777185, w0=72.62673267326743, w1=15.974026658676364\n",
      "SubGD iter. 405/499: loss=5.310579159514617, w0=72.62673267326743, w1=15.972416997955277\n",
      "SubGD iter. 406/499: loss=5.310575458075135, w0=72.62673267326743, w1=15.97080733723419\n",
      "SubGD iter. 407/499: loss=5.310582273979162, w0=72.63366336633673, w1=15.975276037041594\n",
      "SubGD iter. 408/499: loss=5.310623564918673, w0=72.62673267326743, w1=15.973149937255856\n",
      "SubGD iter. 409/499: loss=5.310577143480284, w0=72.62673267326743, w1=15.97154027653477\n",
      "SubGD iter. 410/499: loss=5.3105775949995735, w0=72.63366336633673, w1=15.976008976342174\n",
      "SubGD iter. 411/499: loss=5.310625791064514, w0=72.62673267326743, w1=15.973882876556436\n",
      "SubGD iter. 412/499: loss=5.31057882888543, w0=72.62673267326743, w1=15.972273215835349\n",
      "SubGD iter. 413/499: loss=5.310575127445949, w0=72.62673267326743, w1=15.970663555114262\n",
      "SubGD iter. 414/499: loss=5.310583191863635, w0=72.63366336633673, w1=15.975132254921666\n",
      "SubGD iter. 415/499: loss=5.310623128211338, w0=72.62673267326743, w1=15.973006155135929\n",
      "SubGD iter. 416/499: loss=5.310576812851096, w0=72.62673267326743, w1=15.971396494414842\n",
      "SubGD iter. 417/499: loss=5.310578512884048, w0=72.63366336633673, w1=15.975865194222246\n",
      "SubGD iter. 418/499: loss=5.31062535435718, w0=72.62673267326743, w1=15.973739094436509\n",
      "SubGD iter. 419/499: loss=5.310578498256244, w0=72.62673267326743, w1=15.972129433715422\n",
      "SubGD iter. 420/499: loss=5.310574796816762, w0=72.62673267326743, w1=15.970519772994335\n",
      "SubGD iter. 421/499: loss=5.31058410974811, w0=72.63366336633673, w1=15.974988472801739\n",
      "SubGD iter. 422/499: loss=5.310622691504003, w0=72.62673267326743, w1=15.972862373016001\n",
      "SubGD iter. 423/499: loss=5.310576482221909, w0=72.62673267326743, w1=15.971252712294914\n",
      "SubGD iter. 424/499: loss=5.310579430768521, w0=72.63366336633673, w1=15.975721412102319\n",
      "SubGD iter. 425/499: loss=5.310624917649847, w0=72.62673267326743, w1=15.973595312316581\n",
      "SubGD iter. 426/499: loss=5.310578167627056, w0=72.62673267326743, w1=15.971985651595494\n",
      "SubGD iter. 427/499: loss=5.310574751788934, w0=72.63366336633673, w1=15.976454351402898\n",
      "SubGD iter. 428/499: loss=5.310627143795689, w0=72.62673267326743, w1=15.97432825161716\n",
      "SubGD iter. 429/499: loss=5.310579853032204, w0=72.62673267326743, w1=15.972718590896074\n",
      "SubGD iter. 430/499: loss=5.310576151592722, w0=72.62673267326743, w1=15.971108930174987\n",
      "SubGD iter. 431/499: loss=5.310580348652994, w0=72.63366336633673, w1=15.975577629982391\n",
      "SubGD iter. 432/499: loss=5.310624480942511, w0=72.62673267326743, w1=15.973451530196654\n",
      "SubGD iter. 433/499: loss=5.31057783699787, w0=72.62673267326743, w1=15.971841869475567\n",
      "SubGD iter. 434/499: loss=5.3105756696734066, w0=72.63366336633673, w1=15.97631056928297\n",
      "SubGD iter. 435/499: loss=5.310626707088354, w0=72.62673267326743, w1=15.974184469497233\n",
      "SubGD iter. 436/499: loss=5.310579522403018, w0=72.62673267326743, w1=15.972574808776146\n",
      "SubGD iter. 437/499: loss=5.310575820963536, w0=72.62673267326743, w1=15.97096514805506\n",
      "SubGD iter. 438/499: loss=5.3105812665374685, w0=72.63366336633673, w1=15.975433847862464\n",
      "SubGD iter. 439/499: loss=5.310624044235177, w0=72.62673267326743, w1=15.973307748076726\n",
      "SubGD iter. 440/499: loss=5.310577506368682, w0=72.62673267326743, w1=15.97169808735564\n",
      "SubGD iter. 441/499: loss=5.31057658755788, w0=72.63366336633673, w1=15.976166787163043\n",
      "SubGD iter. 442/499: loss=5.310626270381019, w0=72.62673267326743, w1=15.974040687377306\n",
      "SubGD iter. 443/499: loss=5.31057919177383, w0=72.62673267326743, w1=15.972431026656219\n",
      "SubGD iter. 444/499: loss=5.310575490334348, w0=72.62673267326743, w1=15.970821365935132\n",
      "SubGD iter. 445/499: loss=5.310582184421943, w0=72.63366336633673, w1=15.975290065742536\n",
      "SubGD iter. 446/499: loss=5.310623607527842, w0=72.62673267326743, w1=15.973163965956799\n",
      "SubGD iter. 447/499: loss=5.310577175739496, w0=72.62673267326743, w1=15.971554305235712\n",
      "SubGD iter. 448/499: loss=5.310577505442355, w0=72.63366336633673, w1=15.976023005043116\n",
      "SubGD iter. 449/499: loss=5.310625833673684, w0=72.62673267326743, w1=15.973896905257378\n",
      "SubGD iter. 450/499: loss=5.310578861144643, w0=72.62673267326743, w1=15.972287244536291\n",
      "SubGD iter. 451/499: loss=5.310575159705162, w0=72.62673267326743, w1=15.970677583815204\n",
      "SubGD iter. 452/499: loss=5.310583102306416, w0=72.63366336633673, w1=15.975146283622609\n",
      "SubGD iter. 453/499: loss=5.310623170820506, w0=72.62673267326743, w1=15.973020183836871\n",
      "SubGD iter. 454/499: loss=5.310576845110308, w0=72.62673267326743, w1=15.971410523115784\n",
      "SubGD iter. 455/499: loss=5.310578423326827, w0=72.63366336633673, w1=15.975879222923188\n",
      "SubGD iter. 456/499: loss=5.3106253969663495, w0=72.62673267326743, w1=15.97375312313745\n",
      "SubGD iter. 457/499: loss=5.310578530515456, w0=72.62673267326743, w1=15.972143462416364\n",
      "SubGD iter. 458/499: loss=5.310574829075975, w0=72.62673267326743, w1=15.970533801695277\n",
      "SubGD iter. 459/499: loss=5.3105840201908885, w0=72.63366336633673, w1=15.975002501502681\n",
      "SubGD iter. 460/499: loss=5.3106227341131715, w0=72.62673267326743, w1=15.972876401716944\n",
      "SubGD iter. 461/499: loss=5.310576514481122, w0=72.62673267326743, w1=15.971266740995857\n",
      "SubGD iter. 462/499: loss=5.3105793412113025, w0=72.63366336633673, w1=15.97573544080326\n",
      "SubGD iter. 463/499: loss=5.310624960259014, w0=72.62673267326743, w1=15.973609341017523\n",
      "SubGD iter. 464/499: loss=5.310578199886269, w0=72.62673267326743, w1=15.971999680296436\n",
      "SubGD iter. 465/499: loss=5.310574662231715, w0=72.63366336633673, w1=15.97646838010384\n",
      "SubGD iter. 466/499: loss=5.310627186404858, w0=72.62673267326743, w1=15.974342280318103\n",
      "SubGD iter. 467/499: loss=5.310579885291417, w0=72.62673267326743, w1=15.972732619597016\n",
      "SubGD iter. 468/499: loss=5.310576183851936, w0=72.62673267326743, w1=15.97112295887593\n",
      "SubGD iter. 469/499: loss=5.310580259095775, w0=72.63366336633673, w1=15.975591658683333\n",
      "SubGD iter. 470/499: loss=5.310624523551678, w0=72.62673267326743, w1=15.973465558897596\n",
      "SubGD iter. 471/499: loss=5.310577869257083, w0=72.62673267326743, w1=15.971855898176509\n",
      "SubGD iter. 472/499: loss=5.310575580116187, w0=72.63366336633673, w1=15.976324597983913\n",
      "SubGD iter. 473/499: loss=5.310626749697523, w0=72.62673267326743, w1=15.974198498198175\n",
      "SubGD iter. 474/499: loss=5.3105795546622305, w0=72.62673267326743, w1=15.972588837477089\n",
      "SubGD iter. 475/499: loss=5.3105758532227485, w0=72.62673267326743, w1=15.970979176756002\n",
      "SubGD iter. 476/499: loss=5.310581176980249, w0=72.63366336633673, w1=15.975447876563406\n",
      "SubGD iter. 477/499: loss=5.310624086844345, w0=72.62673267326743, w1=15.973321776777668\n",
      "SubGD iter. 478/499: loss=5.3105775386278955, w0=72.62673267326743, w1=15.971712116056581\n",
      "SubGD iter. 479/499: loss=5.3105764980006605, w0=72.63366336633673, w1=15.976180815863986\n",
      "SubGD iter. 480/499: loss=5.310626312990188, w0=72.62673267326743, w1=15.974054716078248\n",
      "SubGD iter. 481/499: loss=5.3105792240330425, w0=72.62673267326743, w1=15.972445055357161\n",
      "SubGD iter. 482/499: loss=5.3105755225935605, w0=72.62673267326743, w1=15.970835394636074\n",
      "SubGD iter. 483/499: loss=5.310582094864723, w0=72.63366336633673, w1=15.975304094443478\n",
      "SubGD iter. 484/499: loss=5.31062365013701, w0=72.62673267326743, w1=15.97317799465774\n",
      "SubGD iter. 485/499: loss=5.3105772079987075, w0=72.62673267326743, w1=15.971568333936654\n",
      "SubGD iter. 486/499: loss=5.3105774158851355, w0=72.63366336633673, w1=15.976037033744058\n",
      "SubGD iter. 487/499: loss=5.310625876282853, w0=72.62673267326743, w1=15.97391093395832\n",
      "SubGD iter. 488/499: loss=5.310578893403856, w0=72.62673267326743, w1=15.972301273237234\n",
      "SubGD iter. 489/499: loss=5.310575191964374, w0=72.62673267326743, w1=15.970691612516147\n",
      "SubGD iter. 490/499: loss=5.3105830127491975, w0=72.63366336633673, w1=15.97516031232355\n",
      "SubGD iter. 491/499: loss=5.310623213429675, w0=72.62673267326743, w1=15.973034212537813\n",
      "SubGD iter. 492/499: loss=5.310576877369521, w0=72.62673267326743, w1=15.971424551816726\n",
      "SubGD iter. 493/499: loss=5.310578333769609, w0=72.63366336633673, w1=15.97589325162413\n",
      "SubGD iter. 494/499: loss=5.310625439575518, w0=72.62673267326743, w1=15.973767151838393\n",
      "SubGD iter. 495/499: loss=5.310578562774669, w0=72.62673267326743, w1=15.972157491117306\n",
      "SubGD iter. 496/499: loss=5.310574861335188, w0=72.62673267326743, w1=15.97054783039622\n",
      "SubGD iter. 497/499: loss=5.310583930633672, w0=72.63366336633673, w1=15.975016530203623\n",
      "SubGD iter. 498/499: loss=5.310622776722341, w0=72.62673267326743, w1=15.972890430417886\n",
      "SubGD iter. 499/499: loss=5.310576546740335, w0=72.62673267326743, w1=15.971280769696799\n",
      "SubGD: execution time=0.029 seconds\n"
     ]
    }
   ],
   "source": [
    "# Define the parameters of the algorithm.\n",
    "max_iters = 500\n",
    "gamma = 0.7\n",
    "batch_size = 1\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([0, 0])\n",
    "\n",
    "# Start SubSGD.\n",
    "start_time = datetime.datetime.now()\n",
    "subgd_losses, subgd_ws = subgradient_descent(y, tx, w_initial, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"SubGD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "314b192dc336490e9ce46bf344229b14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='n_iter', max=501, min=1), Output()), _dom_classes=('widg…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_figure(n_iter)>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ipywidgets import IntSlider, interact\n",
    "\n",
    "\n",
    "def plot_figure(n_iter):\n",
    "    fig = gradient_descent_visualization(\n",
    "        subgd_losses,\n",
    "        subgd_ws,\n",
    "        grid_losses,\n",
    "        grid_w0,\n",
    "        grid_w1,\n",
    "        mean_x,\n",
    "        std_x,\n",
    "        height,\n",
    "        weight,\n",
    "        n_iter,\n",
    "    )\n",
    "    fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "\n",
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(subgd_ws)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Subgradient Descent\n",
    "\n",
    "**NB** for the computation of the subgradient you can reuse the `compute_subgradient` method that you implemented above, just making sure that you pass in a minibatch as opposed to the full data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stochastic_subgradient_descent(y, tx, initial_w, batch_size, max_iters, gamma):\n",
    "    \"\"\"The Stochastic SubGradient Descent algorithm (SubSGD).\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        initial_w: numpy array of shape=(2, ). The initial guess (or the initialization) for the model parameters\n",
    "        batch_size: a scalar denoting the number of data points in a mini-batch used for computing the stochastic subgradient\n",
    "        max_iters: a scalar denoting the total number of iterations of SubSGD\n",
    "        gamma: a scalar denoting the stepsize\n",
    "\n",
    "    Returns:\n",
    "        losses: a list of length max_iters containing the loss value (scalar) for each iteration of SubSGD\n",
    "        ws: a list of length max_iters containing the model parameters as numpy arrays of shape (2, ), for each iteration of SubSGD\n",
    "    \"\"\"\n",
    "\n",
    "    # Define parameters to store w and loss\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "\n",
    "    for n_iter in range(max_iters):\n",
    "\n",
    "        ### SOLUTION\n",
    "        for y_batch, tx_batch in batch_iter(\n",
    "            y, tx, batch_size=batch_size, num_batches=1\n",
    "        ):\n",
    "            # compute a stochastic subgradient and loss\n",
    "            grad, err = compute_subgradient_mae(y_batch, tx_batch, w)\n",
    "            # update w through the stochastic subgradient update\n",
    "            w = w - gamma * grad\n",
    "            # calculate loss\n",
    "            loss = calculate_mae(err)\n",
    "            # store w and loss\n",
    "            ws.append(w)\n",
    "            losses.append(loss)\n",
    "\n",
    "        ### TEMPLATE\n",
    "        # # ***************************************************\n",
    "        # # INSERT YOUR CODE HERE\n",
    "        # # TODO: implement stochastic subgradient descent.\n",
    "        # # ***************************************************\n",
    "        # raise NotImplementedError\n",
    "        ### END SOLUTION\n",
    "\n",
    "        print(\n",
    "            \"SubSGD iter. {bi}/{ti}: loss={l}, w0={w0}, w1={w1}\".format(\n",
    "                bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]\n",
    "            )\n",
    "        )\n",
    "    return losses, ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SubSGD iter. 0/499: loss=65.54668940257886, w0=0.7, w1=-0.3755858382865697\n",
      "SubSGD iter. 1/499: loss=61.164956325951806, w0=1.4, w1=-0.8065965230351835\n",
      "SubSGD iter. 2/499: loss=94.48253686328121, w0=2.0999999999999996, w1=-0.14345220599213582\n",
      "SubSGD iter. 3/499: loss=75.39413419310632, w0=2.8, w1=0.02235405357536127\n",
      "SubSGD iter. 4/499: loss=87.24017878568914, w0=3.5, w1=0.9020841834078396\n",
      "SubSGD iter. 5/499: loss=58.11081725494887, w0=4.2, w1=0.22373718743329662\n",
      "SubSGD iter. 6/499: loss=50.86968622452161, w0=4.9, w1=-0.3343865163157427\n",
      "SubSGD iter. 7/499: loss=76.2587196339863, w0=5.6000000000000005, w1=-0.13786643353723996\n",
      "SubSGD iter. 8/499: loss=50.16884214971571, w0=6.300000000000001, w1=-0.6733861163634454\n",
      "SubSGD iter. 9/499: loss=51.46904134396671, w0=7.000000000000001, w1=-1.2003431521322931\n",
      "SubSGD iter. 10/499: loss=104.46339382150933, w0=7.700000000000001, w1=0.3397191862548927\n",
      "SubSGD iter. 11/499: loss=51.093336919155774, w0=8.4, w1=0.05979019381842954\n",
      "SubSGD iter. 12/499: loss=84.56664898959912, w0=9.1, w1=0.9389625060492361\n",
      "SubSGD iter. 13/499: loss=67.42518526814949, w0=9.799999999999999, w1=1.274190561352491\n",
      "SubSGD iter. 14/499: loss=56.56621954615177, w0=10.499999999999998, w1=1.0900297830341712\n",
      "SubSGD iter. 15/499: loss=45.4900675188849, w0=11.199999999999998, w1=0.5815286652943378\n",
      "SubSGD iter. 16/499: loss=49.182729264437654, w0=11.899999999999997, w1=0.234444679674347\n",
      "SubSGD iter. 17/499: loss=37.43434055552018, w0=12.599999999999996, w1=-0.9495575207195445\n",
      "SubSGD iter. 18/499: loss=84.48229744746415, w0=13.299999999999995, w1=0.24518131638488094\n",
      "SubSGD iter. 19/499: loss=63.57332230357897, w0=13.999999999999995, w1=0.33370882987400685\n",
      "SubSGD iter. 20/499: loss=68.17046183058324, w0=14.699999999999994, w1=0.7122486132307383\n",
      "SubSGD iter. 21/499: loss=75.10390778162443, w0=15.399999999999993, w1=1.3269095943611249\n",
      "SubSGD iter. 22/499: loss=74.02219741052035, w0=16.099999999999994, w1=1.9503977166473025\n",
      "SubSGD iter. 23/499: loss=43.0854696559051, w0=16.799999999999994, w1=1.5393225321050767\n",
      "SubSGD iter. 24/499: loss=40.65562426541808, w0=17.499999999999993, w1=0.7394917980423672\n",
      "SubSGD iter. 25/499: loss=60.443500826234896, w0=18.199999999999992, w1=0.9419286880833663\n",
      "SubSGD iter. 26/499: loss=75.43481841702273, w0=18.89999999999999, w1=1.7981507405879045\n",
      "SubSGD iter. 27/499: loss=82.46076481973931, w0=19.59999999999999, w1=3.1635201932515153\n",
      "SubSGD iter. 28/499: loss=65.08019919785565, w0=20.29999999999999, w1=3.8977959367433144\n",
      "SubSGD iter. 29/499: loss=55.23853728130845, w0=20.99999999999999, w1=3.5156699521929795\n",
      "SubSGD iter. 30/499: loss=30.874582001883315, w0=21.69999999999999, w1=2.835070774536897\n",
      "SubSGD iter. 31/499: loss=49.52444925756032, w0=22.399999999999988, w1=2.4987078168608203\n",
      "SubSGD iter. 32/499: loss=56.22348156079507, w0=23.099999999999987, w1=2.5250804674856413\n",
      "SubSGD iter. 33/499: loss=55.52424727007002, w0=23.799999999999986, w1=3.0207067279544737\n",
      "SubSGD iter. 34/499: loss=53.005250165250075, w0=24.499999999999986, w1=3.3092039925118892\n",
      "SubSGD iter. 35/499: loss=63.023554722509026, w0=25.199999999999985, w1=3.923864973642276\n",
      "SubSGD iter. 36/499: loss=63.15217912542266, w0=25.899999999999984, w1=4.719034698759623\n",
      "SubSGD iter. 37/499: loss=34.469960815075524, w0=26.599999999999984, w1=3.7422372198177043\n",
      "SubSGD iter. 38/499: loss=62.47927044163808, w0=27.299999999999983, w1=4.090204266208119\n",
      "SubSGD iter. 39/499: loss=57.47553327652197, w0=27.999999999999982, w1=4.815224456288923\n",
      "SubSGD iter. 40/499: loss=58.201137556983845, w0=28.69999999999998, w1=5.429885437419309\n",
      "SubSGD iter. 41/499: loss=60.782145544774586, w0=29.39999999999998, w1=6.309133403315482\n",
      "SubSGD iter. 42/499: loss=43.09558482965896, w0=30.09999999999998, w1=6.2088563040752724\n",
      "SubSGD iter. 43/499: loss=54.80613197738849, w0=30.79999999999998, w1=6.4869832728769605\n",
      "SubSGD iter. 44/499: loss=48.62761015781536, w0=31.49999999999998, w1=7.015039088387609\n",
      "SubSGD iter. 45/499: loss=54.720198364374134, w0=32.19999999999998, w1=7.733202412418797\n",
      "SubSGD iter. 46/499: loss=31.074703632348303, w0=32.899999999999984, w1=7.388025324693799\n",
      "SubSGD iter. 47/499: loss=30.45953100272461, w0=33.59999999999999, w1=6.603792596654475\n",
      "SubSGD iter. 48/499: loss=28.82082971471825, w0=34.29999999999999, w1=6.033832472118489\n",
      "SubSGD iter. 49/499: loss=52.1032041399486, w0=34.99999999999999, w1=6.5870383881946015\n",
      "SubSGD iter. 50/499: loss=44.888345790764504, w0=35.699999999999996, w1=7.2061717841392685\n",
      "SubSGD iter. 51/499: loss=31.210587794722436, w0=36.4, w1=6.8558348507708775\n",
      "SubSGD iter. 52/499: loss=23.686434258845516, w0=37.1, w1=6.308672548537007\n",
      "SubSGD iter. 53/499: loss=109.67318689942957, w0=37.800000000000004, w1=3.538372851648018\n",
      "SubSGD iter. 54/499: loss=21.13291109640336, w0=38.50000000000001, w1=3.2531983570987237\n",
      "SubSGD iter. 55/499: loss=27.227759977028484, w0=39.20000000000001, w1=3.1326938056196814\n",
      "SubSGD iter. 56/499: loss=22.38672574066843, w0=39.90000000000001, w1=2.915436801439931\n",
      "SubSGD iter. 57/499: loss=33.814976179034026, w0=40.600000000000016, w1=3.3389780544939467\n",
      "SubSGD iter. 58/499: loss=46.54214244893091, w0=41.30000000000002, w1=3.9553650033311962\n",
      "SubSGD iter. 59/499: loss=16.90182240487374, w0=42.00000000000002, w1=3.33054890352025\n",
      "SubSGD iter. 60/499: loss=15.54680802714067, w0=42.700000000000024, w1=2.772425199771211\n",
      "SubSGD iter. 61/499: loss=28.97682067724468, w0=43.40000000000003, w1=2.342093611310475\n",
      "SubSGD iter. 62/499: loss=16.63908171356111, w0=44.10000000000003, w1=1.815136575541627\n",
      "SubSGD iter. 63/499: loss=32.5747733958899, w0=44.80000000000003, w1=1.903664089030753\n",
      "SubSGD iter. 64/499: loss=43.34548740594292, w0=45.500000000000036, w1=2.5498113224179737\n",
      "SubSGD iter. 65/499: loss=41.36377834102488, w0=46.20000000000004, w1=3.429541452250452\n",
      "SubSGD iter. 66/499: loss=39.810421141793505, w0=46.90000000000004, w1=3.70766842105214\n",
      "SubSGD iter. 67/499: loss=39.84178852133414, w0=47.600000000000044, w1=3.951981144761274\n",
      "SubSGD iter. 68/499: loss=15.797656491209594, w0=48.30000000000005, w1=3.556544352818268\n",
      "SubSGD iter. 69/499: loss=32.92084301440646, w0=49.00000000000005, w1=4.1877096928131206\n",
      "SubSGD iter. 70/499: loss=42.04240339724674, w0=49.70000000000005, w1=5.066957658709293\n",
      "SubSGD iter. 71/499: loss=26.205929291018364, w0=50.400000000000055, w1=5.296079350996051\n",
      "SubSGD iter. 72/499: loss=33.300493434301565, w0=51.10000000000006, w1=6.201254454183114\n",
      "SubSGD iter. 73/499: loss=29.131188219856128, w0=51.80000000000006, w1=6.73095542824523\n",
      "SubSGD iter. 74/499: loss=5.374672871703524, w0=52.500000000000064, w1=5.605023014744059\n",
      "SubSGD iter. 75/499: loss=30.560520708737393, w0=53.20000000000007, w1=6.366379965363128\n",
      "SubSGD iter. 76/499: loss=5.486005808525299, w0=53.90000000000007, w1=5.526352393903317\n",
      "SubSGD iter. 77/499: loss=89.77709934621255, w0=54.60000000000007, w1=2.756052697014328\n",
      "SubSGD iter. 78/499: loss=5.0585907555217915, w0=55.300000000000075, w1=2.344977512472102\n",
      "SubSGD iter. 79/499: loss=32.556854887536545, w0=56.00000000000008, w1=2.977251940953793\n",
      "SubSGD iter. 80/499: loss=2.022869507533514, w0=55.300000000000075, w1=3.8660837651202864\n",
      "SubSGD iter. 81/499: loss=8.228376733311997, w0=56.00000000000008, w1=3.4402976178295472\n",
      "SubSGD iter. 82/499: loss=4.03325377104273, w0=56.70000000000008, w1=3.160368625393084\n",
      "SubSGD iter. 83/499: loss=29.762196543480663, w0=57.400000000000084, w1=4.065543728580147\n",
      "SubSGD iter. 84/499: loss=12.470473523112538, w0=58.10000000000009, w1=4.243895849959961\n",
      "SubSGD iter. 85/499: loss=26.13203749833187, w0=58.80000000000009, w1=4.710477975926536\n",
      "SubSGD iter. 86/499: loss=0.4273142877868352, w0=59.50000000000009, w1=4.438997793943956\n",
      "SubSGD iter. 87/499: loss=2.0300611037503344, w0=58.80000000000009, w1=5.382504277481077\n",
      "SubSGD iter. 88/499: loss=0.13479004679857098, w0=59.50000000000009, w1=4.835341975247207\n",
      "SubSGD iter. 89/499: loss=81.44237655571231, w0=60.200000000000095, w1=2.065042278358218\n",
      "SubSGD iter. 90/499: loss=6.915611690571751, w0=59.50000000000009, w1=2.9064607880841624\n",
      "SubSGD iter. 91/499: loss=12.488127879244828, w0=60.200000000000095, w1=2.722995009033743\n",
      "SubSGD iter. 92/499: loss=11.183787524121882, w0=60.9000000000001, w1=2.7294228899120894\n",
      "SubSGD iter. 93/499: loss=26.483256313607676, w0=61.6000000000001, w1=3.37557012329931\n",
      "SubSGD iter. 94/499: loss=6.6687757099111025, w0=62.300000000000104, w1=2.636237323855366\n",
      "SubSGD iter. 95/499: loss=23.98952873824669, w0=63.00000000000011, w1=3.3975942744744354\n",
      "SubSGD iter. 96/499: loss=17.935378932545063, w0=63.70000000000011, w1=4.3445065362168656\n",
      "SubSGD iter. 97/499: loss=22.192425128252268, w0=64.4000000000001, w1=4.990653769604086\n",
      "SubSGD iter. 98/499: loss=73.08496936227338, w0=65.10000000000011, w1=1.6175100920144092\n",
      "SubSGD iter. 99/499: loss=5.653953337384628, w0=64.4000000000001, w1=1.6312652879899\n",
      "SubSGD iter. 100/499: loss=26.43930009203828, w0=65.10000000000011, w1=2.172222339961042\n",
      "SubSGD iter. 101/499: loss=11.856106629710496, w0=65.80000000000011, w1=2.187530006673178\n",
      "SubSGD iter. 102/499: loss=24.524580551703195, w0=66.50000000000011, w1=2.9826997317905253\n",
      "SubSGD iter. 103/499: loss=20.191941485051814, w0=67.20000000000012, w1=3.8878748349775885\n",
      "SubSGD iter. 104/499: loss=4.530341097021918, w0=67.90000000000012, w1=3.551511877301512\n",
      "SubSGD iter. 105/499: loss=16.573265882979797, w0=68.60000000000012, w1=4.297497530888909\n",
      "SubSGD iter. 106/499: loss=11.837504986912535, w0=67.90000000000012, w1=4.9202839377092005\n",
      "SubSGD iter. 107/499: loss=7.70436587507244, w0=68.60000000000012, w1=4.520826165322622\n",
      "SubSGD iter. 108/499: loss=11.087071327152934, w0=67.90000000000012, w1=4.998874367748745\n",
      "SubSGD iter. 109/499: loss=18.39113417603852, w0=68.60000000000012, w1=5.243187091457879\n",
      "SubSGD iter. 110/499: loss=0.2070867600604771, w0=69.30000000000013, w1=4.877266360727482\n",
      "SubSGD iter. 111/499: loss=3.6707915918900795, w0=70.00000000000013, w1=4.446934772266746\n",
      "SubSGD iter. 112/499: loss=17.75799445686623, w0=70.70000000000013, w1=5.242104497384093\n",
      "SubSGD iter. 113/499: loss=0.4598098843025582, w0=70.00000000000013, w1=5.981437296828037\n",
      "SubSGD iter. 114/499: loss=14.414609896031408, w0=70.70000000000013, w1=6.192601691394241\n",
      "SubSGD iter. 115/499: loss=11.003270311473855, w0=70.00000000000013, w1=6.701102809134075\n",
      "SubSGD iter. 116/499: loss=7.068846422178368, w0=70.70000000000013, w1=6.31897682458374\n",
      "SubSGD iter. 117/499: loss=12.321344006630511, w0=71.40000000000013, w1=6.584460526807971\n",
      "SubSGD iter. 118/499: loss=10.594221867935985, w0=72.10000000000014, w1=6.6982808536222835\n",
      "SubSGD iter. 119/499: loss=14.823531802500554, w0=72.80000000000014, w1=7.239237905593425\n",
      "SubSGD iter. 120/499: loss=0.7080008823434412, w0=73.50000000000014, w1=7.5744659608966804\n",
      "SubSGD iter. 121/499: loss=10.278283142129553, w0=74.20000000000014, w1=8.189126942027068\n",
      "SubSGD iter. 122/499: loss=11.409676549348646, w0=73.50000000000014, w1=8.80192511302848\n",
      "SubSGD iter. 123/499: loss=4.18129801315672, w0=74.20000000000014, w1=9.329980928539129\n",
      "SubSGD iter. 124/499: loss=3.348470089415244, w0=73.50000000000014, w1=10.36749512235267\n",
      "SubSGD iter. 125/499: loss=5.54885472241304, w0=72.80000000000014, w1=11.57891801027885\n",
      "SubSGD iter. 126/499: loss=2.120828466885456, w0=73.50000000000014, w1=12.63531924165774\n",
      "SubSGD iter. 127/499: loss=8.039807361648094, w0=72.80000000000014, w1=13.046394426199965\n",
      "SubSGD iter. 128/499: loss=5.626602906659201, w0=73.50000000000014, w1=12.35969979069383\n",
      "SubSGD iter. 129/499: loss=1.4974357148457074, w0=74.20000000000014, w1=12.887755606204479\n",
      "SubSGD iter. 130/499: loss=0.18969920589232458, w0=74.90000000000015, w1=12.760741160976453\n",
      "SubSGD iter. 131/499: loss=6.213462787854411, w0=74.20000000000014, w1=13.704247644513574\n",
      "SubSGD iter. 132/499: loss=0.8119705425345813, w0=73.50000000000014, w1=12.498231354743126\n",
      "SubSGD iter. 133/499: loss=7.166265715239824, w0=72.80000000000014, w1=12.780779173829352\n",
      "SubSGD iter. 134/499: loss=3.39879176477249, w0=72.10000000000014, w1=13.565011901868676\n",
      "SubSGD iter. 135/499: loss=103.39066505808424, w0=72.80000000000014, w1=10.794712204979687\n",
      "SubSGD iter. 136/499: loss=10.28406026155993, w0=73.50000000000014, w1=11.072839173781375\n",
      "SubSGD iter. 137/499: loss=3.613435226751136, w0=74.20000000000014, w1=11.834196124400444\n",
      "SubSGD iter. 138/499: loss=4.81712012494976, w0=73.50000000000014, w1=12.265206809149058\n",
      "SubSGD iter. 139/499: loss=7.978585356333909, w0=74.20000000000014, w1=12.818412725225171\n",
      "SubSGD iter. 140/499: loss=4.048357567323521, w0=74.90000000000015, w1=13.613582450342518\n",
      "SubSGD iter. 141/499: loss=1.9265812472678618, w0=74.20000000000014, w1=14.21734852030903\n",
      "SubSGD iter. 142/499: loss=7.20413539253844, w0=73.50000000000014, w1=14.76931995224671\n",
      "SubSGD iter. 143/499: loss=2.321536270764881, w0=74.20000000000014, w1=15.887630172858191\n",
      "SubSGD iter. 144/499: loss=10.23609859697494, w0=73.50000000000014, w1=16.172804667407487\n",
      "SubSGD iter. 145/499: loss=2.3928717863333873, w0=74.20000000000014, w1=16.140375659640128\n",
      "SubSGD iter. 146/499: loss=6.745645909142233, w0=74.90000000000015, w1=15.102861465826587\n",
      "SubSGD iter. 147/499: loss=2.603037844786982, w0=75.60000000000015, w1=15.745791164590383\n",
      "SubSGD iter. 148/499: loss=7.955421512919237, w0=74.90000000000015, w1=16.02833898367661\n",
      "SubSGD iter. 149/499: loss=1.8555467224272846, w0=74.20000000000014, w1=16.929971248089366\n",
      "SubSGD iter. 150/499: loss=0.15031716540666906, w0=73.50000000000014, w1=17.71420397612869\n",
      "SubSGD iter. 151/499: loss=0.5333653599262647, w0=74.20000000000014, w1=17.093152402558097\n",
      "SubSGD iter. 152/499: loss=2.299067283190155, w0=73.50000000000014, w1=17.98198422672459\n",
      "SubSGD iter. 153/499: loss=4.591856900793019, w0=74.20000000000014, w1=18.134877983839175\n",
      "SubSGD iter. 154/499: loss=0.08642842642690596, w0=73.50000000000014, w1=17.518491035001926\n",
      "SubSGD iter. 155/499: loss=0.6013589263039734, w0=74.20000000000014, w1=17.418213935761717\n",
      "SubSGD iter. 156/499: loss=8.437503335542687, w0=74.90000000000015, w1=18.657530236739717\n",
      "SubSGD iter. 157/499: loss=10.045884105479033, w0=74.20000000000014, w1=17.45151394696927\n",
      "SubSGD iter. 158/499: loss=8.587331724447772, w0=73.50000000000014, w1=17.883306525252692\n",
      "SubSGD iter. 159/499: loss=4.250370709929655, w0=74.20000000000014, w1=17.112483459184503\n",
      "SubSGD iter. 160/499: loss=1.4572583264745589, w0=74.90000000000015, w1=17.755413157948297\n",
      "SubSGD iter. 161/499: loss=2.7097138163302787, w0=74.20000000000014, w1=18.10575009131669\n",
      "SubSGD iter. 162/499: loss=7.409573522985674, w0=73.50000000000014, w1=18.583798293742813\n",
      "SubSGD iter. 163/499: loss=12.09912186098461, w0=72.80000000000014, w1=18.035403789985\n",
      "SubSGD iter. 164/499: loss=12.732904432704082, w0=73.50000000000014, w1=17.008471592392468\n",
      "SubSGD iter. 165/499: loss=4.613727134174745, w0=72.80000000000014, w1=17.543991275218673\n",
      "SubSGD iter. 166/499: loss=0.6435201707904739, w0=73.50000000000014, w1=16.887110307465708\n",
      "SubSGD iter. 167/499: loss=5.549647310517869, w0=74.20000000000014, w1=17.42806735943685\n",
      "SubSGD iter. 168/499: loss=2.448849306852523, w0=73.50000000000014, w1=18.269485869162793\n",
      "SubSGD iter. 169/499: loss=11.267785270550036, w0=74.20000000000014, w1=18.61395411603326\n",
      "SubSGD iter. 170/499: loss=2.9205219171679317, w0=73.50000000000014, w1=18.571627559429768\n",
      "SubSGD iter. 171/499: loss=5.033578137149888, w0=72.80000000000014, w1=17.82564190584237\n",
      "SubSGD iter. 172/499: loss=2.9064711606230986, w0=72.10000000000014, w1=18.315325439238958\n",
      "SubSGD iter. 173/499: loss=0.2841249523795568, w0=72.80000000000014, w1=19.171547491743496\n",
      "SubSGD iter. 174/499: loss=14.200425108411906, w0=73.50000000000014, w1=18.552406336329113\n",
      "SubSGD iter. 175/499: loss=3.7593454292693878, w0=72.80000000000014, w1=17.673234024098306\n",
      "SubSGD iter. 176/499: loss=1.064205474899552, w0=73.50000000000014, w1=17.638543164756836\n",
      "SubSGD iter. 177/499: loss=0.9065520459777829, w0=72.80000000000014, w1=18.52737498892333\n",
      "SubSGD iter. 178/499: loss=2.9513149637753955, w0=73.50000000000014, w1=18.161454258192933\n",
      "SubSGD iter. 179/499: loss=9.824986537556178, w0=74.20000000000014, w1=17.779328273642598\n",
      "SubSGD iter. 180/499: loss=7.798167784896037, w0=74.90000000000015, w1=19.0186445746206\n",
      "SubSGD iter. 181/499: loss=6.206531995884411, w0=75.60000000000015, w1=17.834642374226707\n",
      "SubSGD iter. 182/499: loss=4.273399656380306, w0=74.90000000000015, w1=17.792315817623216\n",
      "SubSGD iter. 183/499: loss=3.4399496296125136, w0=74.20000000000014, w1=17.2626148435611\n",
      "SubSGD iter. 184/499: loss=8.959476014761655, w0=73.50000000000014, w1=17.94321402121718\n",
      "SubSGD iter. 185/499: loss=2.1541366659381396, w0=72.80000000000014, w1=17.413513047155064\n",
      "SubSGD iter. 186/499: loss=0.7649431221062386, w0=72.10000000000014, w1=18.221626788196996\n",
      "SubSGD iter. 187/499: loss=6.294616822965651, w0=72.80000000000014, w1=17.543279792222453\n",
      "SubSGD iter. 188/499: loss=125.1732709413345, w0=73.50000000000014, w1=14.170136114632776\n",
      "SubSGD iter. 189/499: loss=3.937096428107367, w0=74.20000000000014, w1=15.182574452499196\n",
      "SubSGD iter. 190/499: loss=7.498833139544985, w0=74.90000000000015, w1=14.800448467948861\n",
      "SubSGD iter. 191/499: loss=3.4005934709956307, w0=74.20000000000014, w1=13.594432178178414\n",
      "SubSGD iter. 192/499: loss=5.985597211065922, w0=73.50000000000014, w1=14.720364591679584\n",
      "SubSGD iter. 193/499: loss=2.3458849786375566, w0=72.80000000000014, w1=14.455579482488815\n",
      "SubSGD iter. 194/499: loss=0.70750627795762, w0=72.10000000000014, w1=14.1670822179314\n",
      "SubSGD iter. 195/499: loss=4.783255705985233, w0=71.40000000000013, w1=14.791898317742346\n",
      "SubSGD iter. 196/499: loss=6.8726890707911465, w0=70.70000000000013, w1=15.26994652016847\n",
      "SubSGD iter. 197/499: loss=7.240043935152698, w0=71.40000000000013, w1=14.4161002497432\n",
      "SubSGD iter. 198/499: loss=11.628777455307429, w0=72.10000000000014, w1=14.856024692627383\n",
      "SubSGD iter. 199/499: loss=7.501613403274739, w0=71.40000000000013, w1=15.135953685063846\n",
      "SubSGD iter. 200/499: loss=4.616916179465278, w0=70.70000000000013, w1=15.971992542336663\n",
      "SubSGD iter. 201/499: loss=4.885009505350382, w0=70.00000000000013, w1=15.548451289282648\n",
      "SubSGD iter. 202/499: loss=1.6888356426655804, w0=70.70000000000013, w1=16.282727032774446\n",
      "SubSGD iter. 203/499: loss=2.7224845073900212, w0=71.40000000000013, w1=16.791912876720698\n",
      "SubSGD iter. 204/499: loss=3.3331652154032625, w0=70.70000000000013, w1=17.074460695806923\n",
      "SubSGD iter. 205/499: loss=0.24634870581510881, w0=71.40000000000013, w1=16.50450057127094\n",
      "SubSGD iter. 206/499: loss=1.51056755633941, w0=72.10000000000014, w1=15.414705548197718\n",
      "SubSGD iter. 207/499: loss=6.658811385898176, w0=71.40000000000013, w1=15.44000331358656\n",
      "SubSGD iter. 208/499: loss=1.9200026077251096, w0=70.70000000000013, w1=14.944377053117728\n",
      "SubSGD iter. 209/499: loss=3.5721966386126383, w0=71.40000000000013, w1=15.823549365348535\n",
      "SubSGD iter. 210/499: loss=5.110079252988683, w0=72.10000000000014, w1=16.447037487634713\n",
      "SubSGD iter. 211/499: loss=11.283885351455254, w0=72.80000000000014, w1=17.57216744776502\n",
      "SubSGD iter. 212/499: loss=1.7977225604737157, w0=73.50000000000014, w1=17.821765910452196\n",
      "SubSGD iter. 213/499: loss=0.19583432563968017, w0=74.20000000000014, w1=18.27194565665849\n",
      "SubSGD iter. 214/499: loss=6.449855401687813, w0=73.50000000000014, w1=17.916918179128665\n",
      "SubSGD iter. 215/499: loss=12.205372263922214, w0=72.80000000000014, w1=16.970005917386235\n",
      "SubSGD iter. 216/499: loss=0.7176948365604261, w0=72.10000000000014, w1=16.44030494332412\n",
      "SubSGD iter. 217/499: loss=1.360479825726273, w0=72.80000000000014, w1=16.81884472668085\n",
      "SubSGD iter. 218/499: loss=0.7406596412338757, w0=72.10000000000014, w1=16.589723034394094\n",
      "SubSGD iter. 219/499: loss=2.890300125050274, w0=72.80000000000014, w1=17.221997462875784\n",
      "SubSGD iter. 220/499: loss=0.12645214504236435, w0=73.50000000000014, w1=17.11182334599504\n",
      "SubSGD iter. 221/499: loss=8.15527456404142, w0=74.20000000000014, w1=16.761885414514726\n",
      "SubSGD iter. 222/499: loss=2.9748554128231675, w0=74.90000000000015, w1=15.785087935572808\n",
      "SubSGD iter. 223/499: loss=3.6684033298449563, w0=75.60000000000015, w1=14.931241665147539\n",
      "SubSGD iter. 224/499: loss=9.201339763969756, w0=74.90000000000015, w1=15.478403967381409\n",
      "SubSGD iter. 225/499: loss=7.9514707194044405, w0=74.20000000000014, w1=14.31007316686394\n",
      "SubSGD iter. 226/499: loss=6.939717273824485, w0=73.50000000000014, w1=14.131721045484126\n",
      "SubSGD iter. 227/499: loss=6.398621053799779, w0=72.80000000000014, w1=14.478805031104116\n",
      "SubSGD iter. 228/499: loss=7.538998874523259, w0=73.50000000000014, w1=14.128867099623799\n",
      "SubSGD iter. 229/499: loss=6.40003613794827, w0=72.80000000000014, w1=14.47595108524379\n",
      "SubSGD iter. 230/499: loss=0.8655059541836891, w0=73.50000000000014, w1=14.37567398600358\n",
      "SubSGD iter. 231/499: loss=0.08103858554322585, w0=74.20000000000014, w1=14.009753255273182\n",
      "SubSGD iter. 232/499: loss=8.138453278726303, w0=73.50000000000014, w1=14.567876959022222\n",
      "SubSGD iter. 233/499: loss=5.45292355705179, w0=72.80000000000014, w1=15.072894690846544\n",
      "SubSGD iter. 234/499: loss=3.464309207743014, w0=72.10000000000014, w1=15.961726515013037\n",
      "SubSGD iter. 235/499: loss=1.9179218212694593, w0=71.40000000000013, w1=16.357163306956043\n",
      "SubSGD iter. 236/499: loss=2.7891183008168383, w0=70.70000000000013, w1=16.437977612444755\n",
      "SubSGD iter. 237/499: loss=9.881900362517968, w0=71.40000000000013, w1=17.61878231314465\n",
      "SubSGD iter. 238/499: loss=7.338760570404581, w0=72.10000000000014, w1=16.434780112750758\n",
      "SubSGD iter. 239/499: loss=3.9681895792258928, w0=71.40000000000013, w1=16.87497536745773\n",
      "SubSGD iter. 240/499: loss=14.054020395928333, w0=72.10000000000014, w1=17.219443614328195\n",
      "SubSGD iter. 241/499: loss=1.2814530342098323, w0=72.80000000000014, w1=17.38524987389569\n",
      "SubSGD iter. 242/499: loss=1.7947188572190385, w0=72.10000000000014, w1=16.660229683814887\n",
      "SubSGD iter. 243/499: loss=1.3289463979049003, w0=71.40000000000013, w1=15.819039515590244\n",
      "SubSGD iter. 244/499: loss=1.2274727594740114, w0=72.10000000000014, w1=16.54405970567105\n",
      "SubSGD iter. 245/499: loss=6.305927875751678, w0=72.80000000000014, w1=16.696953462785633\n",
      "SubSGD iter. 246/499: loss=8.923984931495731, w0=72.10000000000014, w1=16.96843364476821\n",
      "SubSGD iter. 247/499: loss=1.7292767165807703, w0=72.80000000000014, w1=16.93374278542674\n",
      "SubSGD iter. 248/499: loss=2.3712345165594826, w0=73.50000000000014, w1=17.55012973426399\n",
      "SubSGD iter. 249/499: loss=2.682890967489911, w0=72.80000000000014, w1=17.261632469706573\n",
      "SubSGD iter. 250/499: loss=2.3952497225333076, w0=73.50000000000014, w1=17.993597063214207\n",
      "SubSGD iter. 251/499: loss=5.165791875550845, w0=72.80000000000014, w1=18.338774150939205\n",
      "SubSGD iter. 252/499: loss=10.461680459883794, w0=73.50000000000014, w1=17.939316378552626\n",
      "SubSGD iter. 253/499: loss=2.1797289280503236, w0=72.80000000000014, w1=17.896989821949134\n",
      "SubSGD iter. 254/499: loss=1.022608158041436, w0=73.50000000000014, w1=18.776237787845307\n",
      "SubSGD iter. 255/499: loss=3.3326253455507526, w0=72.80000000000014, w1=18.162323374467725\n",
      "SubSGD iter. 256/499: loss=2.5995673959211842, w0=72.10000000000014, w1=17.43730318438692\n",
      "SubSGD iter. 257/499: loss=9.030061715502868, w0=72.80000000000014, w1=17.8772276272711\n",
      "SubSGD iter. 258/499: loss=1.8434348498247175, w0=73.50000000000014, w1=17.965755140760226\n",
      "SubSGD iter. 259/499: loss=4.995023179111669, w0=74.20000000000014, w1=17.62939218308415\n",
      "SubSGD iter. 260/499: loss=8.063622415087465, w0=74.90000000000015, w1=18.86870848406215\n",
      "SubSGD iter. 261/499: loss=2.529840745897225, w0=75.60000000000015, w1=18.07433913957137\n",
      "SubSGD iter. 262/499: loss=4.998938081041985, w0=74.90000000000015, w1=17.785841875013954\n",
      "SubSGD iter. 263/499: loss=6.431975971411397, w0=74.20000000000014, w1=18.06838969410018\n",
      "SubSGD iter. 264/499: loss=1.5138295754212123, w0=73.50000000000014, w1=17.21216764159564\n",
      "SubSGD iter. 265/499: loss=5.3692124047744585, w0=72.80000000000014, w1=17.332672193074682\n",
      "SubSGD iter. 266/499: loss=10.418963750821263, w0=72.10000000000014, w1=16.78427768931687\n",
      "SubSGD iter. 267/499: loss=2.934955316481634, w0=71.40000000000013, w1=17.273961222713456\n",
      "SubSGD iter. 268/499: loss=10.336329125746367, w0=72.10000000000014, w1=16.92402329123314\n",
      "SubSGD iter. 269/499: loss=3.670744424842269, w0=71.40000000000013, w1=16.42839703076431\n",
      "SubSGD iter. 270/499: loss=5.8803973378778664, w0=72.10000000000014, w1=16.172965149662737\n",
      "SubSGD iter. 271/499: loss=1.6393521603154753, w0=71.40000000000013, w1=16.907079472663725\n",
      "SubSGD iter. 272/499: loss=7.424183829332463, w0=70.70000000000013, w1=16.64636725128186\n",
      "SubSGD iter. 273/499: loss=1.415795117401501, w0=71.40000000000013, w1=16.875488943568616\n",
      "SubSGD iter. 274/499: loss=2.0881408159794717, w0=72.10000000000014, w1=15.785693920495396\n",
      "SubSGD iter. 275/499: loss=1.4246754750871133, w0=71.40000000000013, w1=14.727778613667837\n",
      "SubSGD iter. 276/499: loss=1.3437812251904546, w0=72.10000000000014, w1=14.956900305954596\n",
      "SubSGD iter. 277/499: loss=8.441305470664531, w0=72.80000000000014, w1=15.497857357925739\n",
      "SubSGD iter. 278/499: loss=2.9551013790345024, w0=73.50000000000014, w1=14.819510361951195\n",
      "SubSGD iter. 279/499: loss=0.7312996568335919, w0=74.20000000000014, w1=14.122925610710432\n",
      "SubSGD iter. 280/499: loss=3.4463074052205087, w0=74.90000000000015, w1=14.739312559547681\n",
      "SubSGD iter. 281/499: loss=3.649159584480401, w0=74.20000000000014, w1=13.65543716285638\n",
      "SubSGD iter. 282/499: loss=3.857955659491111, w0=74.90000000000015, w1=14.271824111693629\n",
      "SubSGD iter. 283/499: loss=1.8223906798017069, w0=74.20000000000014, w1=15.45582631208752\n",
      "SubSGD iter. 284/499: loss=0.5123977335171332, w0=73.50000000000014, w1=15.25338942204652\n",
      "SubSGD iter. 285/499: loss=2.780956569489831, w0=72.80000000000014, w1=14.047373132276073\n",
      "SubSGD iter. 286/499: loss=7.02834381947433, w0=72.10000000000014, w1=13.089238041374266\n",
      "SubSGD iter. 287/499: loss=0.5415371231348871, w0=72.80000000000014, w1=14.145639272753156\n",
      "SubSGD iter. 288/499: loss=8.068851434517128, w0=73.50000000000014, w1=13.746181500366577\n",
      "SubSGD iter. 289/499: loss=9.738164989338216, w0=74.20000000000014, w1=13.99049422407571\n",
      "SubSGD iter. 290/499: loss=6.522724103643391, w0=73.50000000000014, w1=14.413733648848225\n",
      "SubSGD iter. 291/499: loss=0.4699754710562303, w0=74.20000000000014, w1=15.50635816317402\n",
      "SubSGD iter. 292/499: loss=11.546599700350939, w0=73.50000000000014, w1=14.900955011730035\n",
      "SubSGD iter. 293/499: loss=0.22638904120331915, w0=74.20000000000014, w1=14.800677912489826\n",
      "SubSGD iter. 294/499: loss=1.4387380801962308, w0=74.90000000000015, w1=15.25085765869612\n",
      "SubSGD iter. 295/499: loss=2.5880210026208914, w0=75.60000000000015, w1=15.416310450628746\n",
      "SubSGD iter. 296/499: loss=9.448789091890902, w0=74.90000000000015, w1=14.99276919757473\n",
      "SubSGD iter. 297/499: loss=2.1664484005698483, w0=75.60000000000015, w1=14.656406239898654\n",
      "SubSGD iter. 298/499: loss=7.065300878651584, w0=74.90000000000015, w1=15.359911158289517\n",
      "SubSGD iter. 299/499: loss=5.292030132372922, w0=74.20000000000014, w1=16.094025481290505\n",
      "SubSGD iter. 300/499: loss=3.992598677097618, w0=74.90000000000015, w1=15.509712943744614\n",
      "SubSGD iter. 301/499: loss=0.3913642725406561, w0=75.60000000000015, w1=15.326247164694195\n",
      "SubSGD iter. 302/499: loss=0.8140234590128017, w0=74.90000000000015, w1=15.350213004497286\n",
      "SubSGD iter. 303/499: loss=0.4741015826896273, w0=75.60000000000015, w1=15.620351454786956\n",
      "SubSGD iter. 304/499: loss=4.478272344384337, w0=74.90000000000015, w1=15.970688388155347\n",
      "SubSGD iter. 305/499: loss=10.30027763158651, w0=75.60000000000015, w1=17.21000468913335\n",
      "SubSGD iter. 306/499: loss=7.305490276143388, w0=74.90000000000015, w1=16.304829585946287\n",
      "SubSGD iter. 307/499: loss=3.0176786535679483, w0=74.20000000000014, w1=16.847846047311386\n",
      "SubSGD iter. 308/499: loss=7.586313240693002, w0=73.50000000000014, w1=16.669493925931572\n",
      "SubSGD iter. 309/499: loss=3.083478953751893, w0=72.80000000000014, w1=16.85365470424989\n",
      "SubSGD iter. 310/499: loss=2.424524027518487, w0=72.10000000000014, w1=17.466452875251303\n",
      "SubSGD iter. 311/499: loss=0.7508267877134358, w0=72.80000000000014, w1=17.03544219050269\n",
      "SubSGD iter. 312/499: loss=6.740514620166543, w0=72.10000000000014, w1=17.513490392928812\n",
      "SubSGD iter. 313/499: loss=1.4614346019517086, w0=71.40000000000013, w1=17.69765117124713\n",
      "SubSGD iter. 314/499: loss=9.880218956808847, w0=72.10000000000014, w1=17.45667125658048\n",
      "SubSGD iter. 315/499: loss=0.5364547669509037, w0=72.80000000000014, w1=17.086403166051685\n",
      "SubSGD iter. 316/499: loss=3.13519620463164, w0=73.50000000000014, w1=17.053974158284326\n",
      "SubSGD iter. 317/499: loss=1.6488142407980035, w0=72.80000000000014, w1=17.94280598245082\n",
      "SubSGD iter. 318/499: loss=5.976577415952448, w0=72.10000000000014, w1=17.24187198501607\n",
      "SubSGD iter. 319/499: loss=3.155369670627664, w0=71.40000000000013, w1=16.63733987916191\n",
      "SubSGD iter. 320/499: loss=0.3705318214499158, w0=72.10000000000014, w1=17.165395694672558\n",
      "SubSGD iter. 321/499: loss=6.3142437286048505, w0=72.80000000000014, w1=16.611288620675978\n",
      "SubSGD iter. 322/499: loss=0.47572311362520736, w0=72.10000000000014, w1=17.042299305424592\n",
      "SubSGD iter. 323/499: loss=0.053531124213463954, w0=71.40000000000013, w1=17.86095767523022\n",
      "SubSGD iter. 324/499: loss=5.2421049949310685, w0=70.70000000000013, w1=18.07821467940997\n",
      "SubSGD iter. 325/499: loss=9.148865284703824, w0=71.40000000000013, w1=17.49390214186408\n",
      "SubSGD iter. 326/499: loss=4.127074611029187, w0=70.70000000000013, w1=16.792968144429327\n",
      "SubSGD iter. 327/499: loss=1.176506577635962, w0=71.40000000000013, w1=18.333030482816515\n",
      "SubSGD iter. 328/499: loss=1.033266386564705, w0=70.70000000000013, w1=18.08274936635446\n",
      "SubSGD iter. 329/499: loss=6.101468549217898, w0=70.00000000000013, w1=16.888010529250035\n",
      "SubSGD iter. 330/499: loss=4.839446498994761, w0=70.70000000000013, w1=16.864044689446946\n",
      "SubSGD iter. 331/499: loss=6.70815569514798, w0=70.00000000000013, w1=16.60333246806508\n",
      "SubSGD iter. 332/499: loss=1.278979079826236, w0=70.70000000000013, w1=15.714500643898587\n",
      "SubSGD iter. 333/499: loss=9.255835120112366, w0=71.40000000000013, w1=16.25545769586973\n",
      "SubSGD iter. 334/499: loss=10.17466938134779, w0=72.10000000000014, w1=15.568763060363594\n",
      "SubSGD iter. 335/499: loss=2.1039375350569856, w0=71.40000000000013, w1=16.095720096132442\n",
      "SubSGD iter. 336/499: loss=9.256316231142563, w0=70.70000000000013, w1=15.490316944688457\n",
      "SubSGD iter. 337/499: loss=2.4401428443475197, w0=70.00000000000013, w1=15.980000478085046\n",
      "SubSGD iter. 338/499: loss=10.066809770129623, w0=69.30000000000013, w1=16.099892291573376\n",
      "SubSGD iter. 339/499: loss=5.668824008524709, w0=70.00000000000013, w1=15.746776540006062\n",
      "SubSGD iter. 340/499: loss=5.5723906353386425, w0=70.70000000000013, w1=14.56277433961217\n",
      "SubSGD iter. 341/499: loss=8.084489195890534, w0=70.00000000000013, w1=15.243373517268253\n",
      "SubSGD iter. 342/499: loss=4.720918797329354, w0=70.70000000000013, w1=14.059371316874362\n",
      "SubSGD iter. 343/499: loss=1.737754287404364, w0=70.00000000000013, w1=15.185303730375534\n",
      "SubSGD iter. 344/499: loss=0.9126652942374278, w0=70.70000000000013, w1=16.090478833562596\n",
      "SubSGD iter. 345/499: loss=1.4830649005998566, w0=71.40000000000013, w1=16.618534649073244\n",
      "SubSGD iter. 346/499: loss=1.8153682055024376, w0=70.70000000000013, w1=16.026216687805064\n",
      "SubSGD iter. 347/499: loss=10.758894134135502, w0=70.00000000000013, w1=16.146108501293394\n",
      "SubSGD iter. 348/499: loss=1.1716688261042947, w0=69.30000000000013, w1=15.791081023763569\n",
      "SubSGD iter. 349/499: loss=12.373126837920083, w0=70.00000000000013, w1=16.971885724463466\n",
      "SubSGD iter. 350/499: loss=4.55548362216409, w0=69.30000000000013, w1=17.25181471689993\n",
      "SubSGD iter. 351/499: loss=12.790343963059627, w0=70.00000000000013, w1=18.376944677030238\n",
      "SubSGD iter. 352/499: loss=3.681959580349968, w0=69.30000000000013, w1=18.594201681209988\n",
      "SubSGD iter. 353/499: loss=0.5943739449258203, w0=68.60000000000012, w1=19.10270279894982\n",
      "SubSGD iter. 354/499: loss=16.73966225388679, w0=69.30000000000013, w1=17.748745904916593\n",
      "SubSGD iter. 355/499: loss=5.2347601155470045, w0=70.00000000000013, w1=17.99834436760377\n",
      "SubSGD iter. 356/499: loss=4.666709186064466, w0=69.30000000000013, w1=17.799583501785246\n",
      "SubSGD iter. 357/499: loss=12.791422919032911, w0=70.00000000000013, w1=18.147550548175662\n",
      "SubSGD iter. 358/499: loss=8.055693962309746, w0=70.70000000000013, w1=18.300444305290245\n",
      "SubSGD iter. 359/499: loss=3.630945086069417, w0=70.00000000000013, w1=17.420714175457768\n",
      "SubSGD iter. 360/499: loss=5.792226172172917, w0=70.70000000000013, w1=17.23724839640735\n",
      "SubSGD iter. 361/499: loss=1.1800785899420703, w0=71.40000000000013, w1=17.766949370469465\n",
      "SubSGD iter. 362/499: loss=1.2270339617063541, w0=72.10000000000014, w1=18.562119095586812\n",
      "SubSGD iter. 363/499: loss=0.5772598658025672, w0=71.40000000000013, w1=18.91884461394265\n",
      "SubSGD iter. 364/499: loss=6.662271482836324, w0=70.70000000000013, w1=19.19032479592523\n",
      "SubSGD iter. 365/499: loss=9.946829909504515, w0=70.00000000000013, w1=18.021993995407758\n",
      "SubSGD iter. 366/499: loss=4.80270496819729, w0=69.30000000000013, w1=16.96559276402887\n",
      "SubSGD iter. 367/499: loss=5.6349028309537985, w0=70.00000000000013, w1=16.59967203329847\n",
      "SubSGD iter. 368/499: loss=0.7721263742648432, w0=70.70000000000013, w1=16.072714997529623\n",
      "SubSGD iter. 369/499: loss=0.4389011795815492, w0=71.40000000000013, w1=16.806990741021423\n",
      "SubSGD iter. 370/499: loss=9.748936411906868, w0=72.10000000000014, w1=17.018155135587627\n",
      "SubSGD iter. 371/499: loss=0.714503004439365, w0=71.40000000000013, w1=16.29313494550682\n",
      "SubSGD iter. 372/499: loss=11.413177971322625, w0=70.70000000000013, w1=16.41302675899515\n",
      "SubSGD iter. 373/499: loss=2.0124114612100072, w0=71.40000000000013, w1=17.95308909738234\n",
      "SubSGD iter. 374/499: loss=2.816945611940831, w0=70.70000000000013, w1=18.364164281924566\n",
      "SubSGD iter. 375/499: loss=8.56793165996406, w0=70.00000000000013, w1=17.195833481407096\n",
      "SubSGD iter. 376/499: loss=4.347536926761045, w0=70.70000000000013, w1=16.41160075336777\n",
      "SubSGD iter. 377/499: loss=2.0989025139093656, w0=71.40000000000013, w1=16.30142663648703\n",
      "SubSGD iter. 378/499: loss=3.425995885251865, w0=70.70000000000013, w1=16.42193118796607\n",
      "SubSGD iter. 379/499: loss=3.953205207590422, w0=71.40000000000013, w1=15.62756184347529\n",
      "SubSGD iter. 380/499: loss=8.913652968799838, w0=72.10000000000014, w1=16.290706160518337\n",
      "SubSGD iter. 381/499: loss=2.2379700598074805, w0=71.40000000000013, w1=15.698388199250159\n",
      "SubSGD iter. 382/499: loss=4.235821194171194, w0=70.70000000000013, w1=16.083672339578285\n",
      "SubSGD iter. 383/499: loss=4.761690264856824, w0=70.00000000000013, w1=14.915341539060817\n",
      "SubSGD iter. 384/499: loss=4.308664095798676, w0=70.70000000000013, w1=15.794513851291624\n",
      "SubSGD iter. 385/499: loss=5.958213060694746, w0=71.40000000000013, w1=15.80982151800376\n",
      "SubSGD iter. 386/499: loss=4.244317897572763, w0=70.70000000000013, w1=16.05384834271291\n",
      "SubSGD iter. 387/499: loss=8.992471595188832, w0=71.40000000000013, w1=16.66684083989647\n",
      "SubSGD iter. 388/499: loss=9.791214510506236, w0=72.10000000000014, w1=16.878005234462673\n",
      "SubSGD iter. 389/499: loss=1.2286212734803712, w0=72.80000000000014, w1=16.859425775563796\n",
      "SubSGD iter. 390/499: loss=6.8837206749566064, w0=72.10000000000014, w1=16.202243072368812\n",
      "SubSGD iter. 391/499: loss=7.890459094023349, w0=71.40000000000013, w1=16.882842250024893\n",
      "SubSGD iter. 392/499: loss=1.1618414938741495, w0=72.10000000000014, w1=16.935002595554604\n",
      "SubSGD iter. 393/499: loss=10.023412658853431, w0=72.80000000000014, w1=15.897488401741063\n",
      "SubSGD iter. 394/499: loss=2.6542105547448074, w0=72.10000000000014, w1=16.29292519368407\n",
      "SubSGD iter. 395/499: loss=3.6244952208651853, w0=71.40000000000013, w1=15.591991196249317\n",
      "SubSGD iter. 396/499: loss=8.94735078674475, w0=72.10000000000014, w1=16.255135513292366\n",
      "SubSGD iter. 397/499: loss=6.4404994408568825, w0=71.40000000000013, w1=16.472392517472116\n",
      "SubSGD iter. 398/499: loss=7.925950968235583, w0=72.10000000000014, w1=17.085385014655674\n",
      "SubSGD iter. 399/499: loss=3.4050458271746322, w0=71.40000000000013, w1=17.166199320144386\n",
      "SubSGD iter. 400/499: loss=5.771729628236869, w0=70.70000000000013, w1=16.509016616949403\n",
      "SubSGD iter. 401/499: loss=2.645741672921993, w0=70.00000000000013, w1=17.067140320698442\n",
      "SubSGD iter. 402/499: loss=0.2820689919630013, w0=70.70000000000013, w1=17.908330488923085\n",
      "SubSGD iter. 403/499: loss=3.9531702985384527, w0=71.40000000000013, w1=18.53181861120926\n",
      "SubSGD iter. 404/499: loss=10.478182175085962, w0=72.10000000000014, w1=18.004629972462034\n",
      "SubSGD iter. 405/499: loss=0.20055330614067657, w0=72.80000000000014, w1=18.114930005922826\n",
      "SubSGD iter. 406/499: loss=3.61162670755337, w0=72.10000000000014, w1=18.555125260629797\n",
      "SubSGD iter. 407/499: loss=8.148717833634592, w0=72.80000000000014, w1=18.751469854147025\n",
      "SubSGD iter. 408/499: loss=3.8606123437484, w0=73.50000000000014, w1=18.91692264607965\n",
      "SubSGD iter. 409/499: loss=3.4491066599911875, w0=72.80000000000014, w1=18.388866830569004\n",
      "SubSGD iter. 410/499: loss=0.5599914628679983, w0=72.10000000000014, w1=18.278566797108212\n",
      "SubSGD iter. 411/499: loss=6.555312400210838, w0=71.40000000000013, w1=18.303864562497054\n",
      "SubSGD iter. 412/499: loss=5.203338340193326, w0=72.10000000000014, w1=18.31917222920919\n",
      "SubSGD iter. 413/499: loss=3.6357391868153144, w0=72.80000000000014, w1=17.229377206135972\n",
      "SubSGD iter. 414/499: loss=1.2759967646978936, w0=72.10000000000014, w1=16.61546279275839\n",
      "SubSGD iter. 415/499: loss=3.3322939204084747, w0=72.80000000000014, w1=17.494710758654563\n",
      "SubSGD iter. 416/499: loss=0.5878797897014607, w0=73.50000000000014, w1=18.3509328111591\n",
      "SubSGD iter. 417/499: loss=1.0197785139041997, w0=74.20000000000014, w1=18.993862509922895\n",
      "SubSGD iter. 418/499: loss=0.1288453780254244, w0=74.90000000000015, w1=18.175204140117266\n",
      "SubSGD iter. 419/499: loss=1.6251215407083848, w0=74.20000000000014, w1=19.06403596428376\n",
      "SubSGD iter. 420/499: loss=1.8006896744158212, w0=73.50000000000014, w1=19.01187561875405\n",
      "SubSGD iter. 421/499: loss=0.4120726493643332, w0=74.20000000000014, w1=19.73003894278524\n",
      "SubSGD iter. 422/499: loss=9.164164941263792, w0=73.50000000000014, w1=18.824863839598176\n",
      "SubSGD iter. 423/499: loss=3.0555482524342352, w0=74.20000000000014, w1=20.00566854029807\n",
      "SubSGD iter. 424/499: loss=0.2152418658988111, w0=73.50000000000014, w1=20.375936630826867\n",
      "SubSGD iter. 425/499: loss=10.885135117351325, w0=72.80000000000014, w1=19.718753927631884\n",
      "SubSGD iter. 426/499: loss=0.49554058228670783, w0=72.10000000000014, w1=20.11419071957489\n",
      "SubSGD iter. 427/499: loss=2.139330693082556, w0=71.40000000000013, w1=19.319020994457542\n",
      "SubSGD iter. 428/499: loss=2.772790820841351, w0=70.70000000000013, w1=18.699887598512877\n",
      "SubSGD iter. 429/499: loss=5.7263995652766795, w0=71.40000000000013, w1=17.573955185011705\n",
      "SubSGD iter. 430/499: loss=3.840006405506628, w0=72.10000000000014, w1=17.44694073978368\n",
      "SubSGD iter. 431/499: loss=1.2051460311530775, w0=71.40000000000013, w1=18.059738910785093\n",
      "SubSGD iter. 432/499: loss=6.055719367766002, w0=72.10000000000014, w1=17.36315415954433\n",
      "SubSGD iter. 433/499: loss=4.090259407420547, w0=72.80000000000014, w1=17.55967424232283\n",
      "SubSGD iter. 434/499: loss=0.306377553422422, w0=72.10000000000014, w1=17.05048839837658\n",
      "SubSGD iter. 435/499: loss=9.486106249855723, w0=72.80000000000014, w1=16.52585547366484\n",
      "SubSGD iter. 436/499: loss=0.8852774482166907, w0=73.50000000000014, w1=15.624223209252083\n",
      "SubSGD iter. 437/499: loss=1.8339913978368259, w0=74.20000000000014, w1=16.636661547118504\n",
      "SubSGD iter. 438/499: loss=2.439796894623143, w0=74.90000000000015, w1=16.65196921383064\n",
      "SubSGD iter. 439/499: loss=2.550674870322972, w0=74.20000000000014, w1=17.082979898579254\n",
      "SubSGD iter. 440/499: loss=0.02110339371804315, w0=74.90000000000015, w1=16.29874717053993\n",
      "SubSGD iter. 441/499: loss=5.981614460662968, w0=75.60000000000015, w1=16.495091764057157\n",
      "SubSGD iter. 442/499: loss=0.8941610193030982, w0=76.30000000000015, w1=17.213255088088346\n",
      "SubSGD iter. 443/499: loss=4.963918714713856, w0=75.60000000000015, w1=16.685199272577698\n",
      "SubSGD iter. 444/499: loss=3.3021771834540914, w0=74.90000000000015, w1=16.15549829851558\n",
      "SubSGD iter. 445/499: loss=4.923561423851922, w0=74.20000000000014, w1=15.563180337247404\n",
      "SubSGD iter. 446/499: loss=5.301144303732642, w0=73.50000000000014, w1=14.368441500142978\n",
      "SubSGD iter. 447/499: loss=0.5406716310302926, w0=74.20000000000014, w1=15.461066014468773\n",
      "SubSGD iter. 448/499: loss=1.8737856351023652, w0=73.50000000000014, w1=16.404572498005894\n",
      "SubSGD iter. 449/499: loss=10.954794431698943, w0=72.80000000000014, w1=15.446437407104087\n",
      "SubSGD iter. 450/499: loss=2.0539699908299127, w0=73.50000000000014, w1=15.319422961876061\n",
      "SubSGD iter. 451/499: loss=108.93387138345159, w0=74.20000000000014, w1=12.549123264987072\n",
      "SubSGD iter. 452/499: loss=5.210100657387201, w0=74.90000000000015, w1=13.163784246117459\n",
      "SubSGD iter. 453/499: loss=2.275107146441286, w0=74.20000000000014, w1=12.875286981560043\n",
      "SubSGD iter. 454/499: loss=4.256894634762773, w0=74.90000000000015, w1=12.350654056848304\n",
      "SubSGD iter. 455/499: loss=8.452547672772958, w0=74.20000000000014, w1=12.855671788672627\n",
      "SubSGD iter. 456/499: loss=7.965126019559591, w0=74.90000000000015, w1=13.39662884064377\n",
      "SubSGD iter. 457/499: loss=1.7066234125176578, w0=74.20000000000014, w1=14.608051728569949\n",
      "SubSGD iter. 458/499: loss=0.93162802346977, w0=73.50000000000014, w1=14.626631187468826\n",
      "SubSGD iter. 459/499: loss=7.296536054511918, w0=74.20000000000014, w1=15.167588239439969\n",
      "SubSGD iter. 460/499: loss=1.5402300049347417, w0=73.50000000000014, w1=14.4062312888209\n",
      "SubSGD iter. 461/499: loss=10.27824398604372, w0=74.20000000000014, w1=14.754198335211314\n",
      "SubSGD iter. 462/499: loss=1.4393602967360621, w0=73.50000000000014, w1=15.548567679702094\n",
      "SubSGD iter. 463/499: loss=3.2763271619660657, w0=74.20000000000014, w1=16.16322866083248\n",
      "SubSGD iter. 464/499: loss=14.468125895215486, w0=73.50000000000014, w1=16.176983856807972\n",
      "SubSGD iter. 465/499: loss=3.6357138668494713, w0=72.80000000000014, w1=15.572451750953812\n",
      "SubSGD iter. 466/499: loss=5.009059629570217, w0=73.50000000000014, w1=16.09663230240553\n",
      "SubSGD iter. 467/499: loss=0.31712718740814694, w0=72.80000000000014, w1=17.040138785942652\n",
      "SubSGD iter. 468/499: loss=9.912866853520235, w0=73.50000000000014, w1=16.658012801392317\n",
      "SubSGD iter. 469/499: loss=7.394627642350102, w0=72.80000000000014, w1=16.000830098197333\n",
      "SubSGD iter. 470/499: loss=6.031561388317009, w0=73.50000000000014, w1=15.146983827772065\n",
      "SubSGD iter. 471/499: loss=2.46781052134277, w0=72.80000000000014, w1=14.267253697939587\n",
      "SubSGD iter. 472/499: loss=6.700139734070774, w0=73.50000000000014, w1=13.742620773227848\n",
      "SubSGD iter. 473/499: loss=7.112194841084374, w0=72.80000000000014, w1=14.278140456054054\n",
      "SubSGD iter. 474/499: loss=5.874623894678685, w0=73.50000000000014, w1=13.750951817306827\n",
      "SubSGD iter. 475/499: loss=3.998335180007757, w0=74.20000000000014, w1=14.178468902524246\n",
      "SubSGD iter. 476/499: loss=5.840646408197273, w0=73.50000000000014, w1=14.259283208012958\n",
      "SubSGD iter. 477/499: loss=0.3494082406847241, w0=74.20000000000014, w1=14.602995829196349\n",
      "SubSGD iter. 478/499: loss=3.8429422926717365, w0=73.50000000000014, w1=13.546594597817458\n",
      "SubSGD iter. 479/499: loss=7.52018111973058, w0=72.80000000000014, w1=13.931878738145587\n",
      "SubSGD iter. 480/499: loss=13.11197254589613, w0=72.10000000000014, w1=13.945633934121076\n",
      "SubSGD iter. 481/499: loss=1.8036735444234182, w0=71.40000000000013, w1=13.610405878817822\n",
      "SubSGD iter. 482/499: loss=2.14970759759386, w0=72.10000000000014, w1=14.81642216858827\n",
      "SubSGD iter. 483/499: loss=4.631747479000111, w0=71.40000000000013, w1=15.368393600525948\n",
      "SubSGD iter. 484/499: loss=0.40437254724128735, w0=70.70000000000013, w1=15.71873053389434\n",
      "SubSGD iter. 485/499: loss=7.895703346347588, w0=71.40000000000013, w1=16.731408875247105\n",
      "SubSGD iter. 486/499: loss=8.501696126414814, w0=70.70000000000013, w1=15.784496613504675\n",
      "SubSGD iter. 487/499: loss=0.40963147857658555, w0=70.00000000000013, w1=15.192178652236498\n",
      "SubSGD iter. 488/499: loss=2.503242304505875, w0=69.30000000000013, w1=15.727698335062703\n",
      "SubSGD iter. 489/499: loss=11.29470736457575, w0=70.00000000000013, w1=15.203065410350964\n",
      "SubSGD iter. 490/499: loss=8.733714697276561, w0=69.30000000000013, w1=15.14675579530566\n",
      "SubSGD iter. 491/499: loss=6.1166930551599705, w0=68.60000000000012, w1=15.827354972961743\n",
      "SubSGD iter. 492/499: loss=2.9271316126304754, w0=69.30000000000013, w1=16.11585223751916\n",
      "SubSGD iter. 493/499: loss=1.7958634422494484, w0=68.60000000000012, w1=16.526927422061387\n",
      "SubSGD iter. 494/499: loss=0.07511024614639439, w0=67.90000000000012, w1=15.32091113229094\n",
      "SubSGD iter. 495/499: loss=4.480513027327717, w0=68.60000000000012, w1=16.162101300515584\n",
      "SubSGD iter. 496/499: loss=0.39028979616282555, w0=69.30000000000013, w1=15.73886187574307\n",
      "SubSGD iter. 497/499: loss=16.310715311345703, w0=70.00000000000013, w1=16.97817817672107\n",
      "SubSGD iter. 498/499: loss=6.6323285764194395, w0=70.70000000000013, w1=16.993485843433206\n",
      "SubSGD iter. 499/499: loss=8.053238136688307, w0=70.00000000000013, w1=16.445091339675393\n",
      "SubSGD: execution time=0.099 seconds\n"
     ]
    }
   ],
   "source": [
    "# Define the parameters of the algorithm.\n",
    "max_iters = 500\n",
    "gamma = 0.7\n",
    "batch_size = 1\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([0, 0])\n",
    "\n",
    "# Start SubSGD.\n",
    "start_time = datetime.datetime.now()\n",
    "subsgd_losses, subsgd_ws = stochastic_subgradient_descent(\n",
    "    y, tx, w_initial, batch_size, max_iters, gamma\n",
    ")\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"SubSGD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abd2fc8e060840678961eba68560037c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='n_iter', max=501, min=1), Output()), _dom_classes=('widg…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_figure(n_iter)>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ipywidgets import IntSlider, interact\n",
    "\n",
    "\n",
    "def plot_figure(n_iter):\n",
    "    fig = gradient_descent_visualization(\n",
    "        subsgd_losses,\n",
    "        subsgd_ws,\n",
    "        grid_losses,\n",
    "        grid_w0,\n",
    "        grid_w1,\n",
    "        mean_x,\n",
    "        std_x,\n",
    "        height,\n",
    "        weight,\n",
    "        n_iter,\n",
    "    )\n",
    "    fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "\n",
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(subsgd_ws)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ada",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
